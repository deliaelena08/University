{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Clinical Triage Model Pipeline: V2 Documentation\n",
    "Here I summarize the significant configuration and code improvements made to the Clinical Triage Prediction Model pipeline.\n",
    "\n",
    "Training Data Size:\n",
    "   - old value: 5000\n",
    "   - new value: 234000\n",
    "   - improvement rationale:\n",
    "      - Massive increase in data used for fine-tuning. This is critical for improving model generalization and predictive power in a clinical setting.\n",
    "\n",
    "Max Token Length:\n",
    "   - old value: 512\n",
    "   - new value: 256\n",
    "   - improvement rationale:\n",
    "      - Reduction in input length. This dramatically lowers the model's VRAM requirement per sample, speeding up training and reducing the risk of memory bottlenecks.\n",
    "\n",
    "Batch Size (BATCH_SIZE)\n",
    "   - old value: 2\n",
    "   - new value: 16\n",
    "   - improvement rationale:\n",
    "      - Increased per-device batch size for faster training throughput.\n",
    "\n",
    "Gradient Accumulation (GRAD_ACCUM_STEPS):\n",
    "   - old value: Implicitly 1\n",
    "   - new value: 2\n",
    "   - improvement rationale:\n",
    "      - Introduced to compensate for potential batch size limits, resulting in an effective batch size of 32 ($16 \\times 2$). This helps stabilize training while conserving memory.\n",
    "\n",
    "Epochs (NUM_EPOCHS):\n",
    "   - old value: 3\n",
    "   - new value: 10\n",
    "   - improvement rationale:\n",
    "      - Allows the model more time to learn complex patterns in the larger dataset.\n",
    "\n",
    "Early Stopping Patience (PATIENCE):\n",
    "   - old value: 1\n",
    "   - new value: 3\n",
    "   - improvement rationale:\n",
    "      - Prevents premature stopping by requiring the validation loss to worsen for three consecutive epochs before halting.\n",
    "   \n",
    "Model Output Cleanup\n",
    "   - old value: None\n",
    "   - new value: Added shutil.rmtree(MODEL_OUTPUT_DIR)\n",
    "   - improvement rationale:\n",
    "      - Ensures a clean slate for each run, deleting old checkpoints and processed data to free up disk space.\n",
    "\n"
   ],
   "id": "acb7f846583d73d3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-01T13:12:43.646808Z",
     "start_time": "2025-12-01T13:12:43.610076Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Hugging Face and Transformers\n",
    "try:\n",
    "    from datasets import Dataset, DatasetDict, load_from_disk\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "        EarlyStoppingCallback,\n",
    "        pipeline\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"ImportError: Hugging Face libraries not found.\")\n",
    "    print(\"Installing missing libraries...\")\n",
    "    os.system(\"pip install torch transformers datasets scikit-learn pandas numpy matplotlib seaborn accelerate\")\n",
    "    print(\"Libraries installed! Please restart the kernel if you see import errors.\")\n",
    "    exit()\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CONFIGURATION",
   "id": "6443c14d355a0f2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:24:11.384806Z",
     "start_time": "2025-11-25T12:24:11.380459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data files \n",
    "TRAIN_FILE = \"data/bigdata_models/train.csv\"\n",
    "VAL_FILE = \"data/bigdata_models/val.csv\"\n",
    "TEST_FILE = \"data/bigdata_models/test.csv\"\n",
    "\n",
    "# Train/Test data size\n",
    "N_TRAIN_ROWS = 232400  \n",
    "N_TEST_ROWS = 23240   \n",
    "\n",
    "# Preprocessing\n",
    "TARGET_COLUMN = 'discharge_disposition'\n",
    "\n",
    "# Model: ClinicalBERT\n",
    "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "MAX_TOKEN_LENGTH = 256\n",
    "PROCESSED_DATA_DIR = \"data/processed_data/tpaul/processed_triage_data\"\n",
    "LABEL_INFO_FILE = \"data/info/tpaul/label_info.json\"\n",
    "\n",
    "# Training Output\n",
    "MODEL_OUTPUT_DIR = \"models/tpaul/triage_classifier_model\"\n",
    "ZIP_NAME = \"triage_classifier_model\"  # Name of the zip file to create\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16          \n",
    "GRAD_ACCUM_STEPS = 2     \n",
    "LEARNING_RATE = 2e-5     \n",
    "NUM_EPOCHS = 10          \n",
    "PATIENCE = 3             \n",
    "\n",
    "# Evaluation\n",
    "CONFUSION_MATRIX_FILE = \"evaluation/tpaul/confusion_matrix.png\""
   ],
   "id": "26cf5ab62435e3ef",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PREPROCESSING",
   "id": "a17cb303841e9bca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:24:11.405491Z",
     "start_time": "2025-11-25T12:24:11.393645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_label_maps() -> Dict[str, Any]:\n",
    "    print(\"Creating complete label mappings...\")\n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_FILE, usecols=[TARGET_COLUMN], nrows=N_TRAIN_ROWS)\n",
    "        df_val = pd.read_csv(VAL_FILE, usecols=[TARGET_COLUMN], nrows=N_TEST_ROWS)\n",
    "        df_test = pd.read_csv(TEST_FILE, usecols=[TARGET_COLUMN], nrows=N_TEST_ROWS)\n",
    "        \n",
    "        all_labels = pd.concat([df_train, df_val, df_test])\n",
    "        all_labels[TARGET_COLUMN] = all_labels[TARGET_COLUMN].fillna('Unknown').astype(str).str.strip()\n",
    "        \n",
    "        unique_labels = all_labels[TARGET_COLUMN].unique()\n",
    "        unique_labels.sort()\n",
    "\n",
    "        label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "        id2label = {i: label for label, i in label2id.items()}\n",
    "        num_labels = len(unique_labels)\n",
    "\n",
    "        print(f\"Found {num_labels} unique labels: {unique_labels}\")\n",
    "\n",
    "        label_info = {'label2id': label2id, 'id2label': id2label, 'num_labels': num_labels}\n",
    "\n",
    "        os.makedirs(os.path.dirname(LABEL_INFO_FILE) or '.', exist_ok=True)\n",
    "        with open(LABEL_INFO_FILE, 'w') as f:\n",
    "            json.dump(label_info, f)\n",
    "\n",
    "        return label_info\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nCRITICAL ERROR: {e}\")\n",
    "        print(f\"Please check that '{TRAIN_FILE}' exists.\")\n",
    "        raise e\n",
    "\n",
    "def preprocess(label2id: Dict[str, int]):\n",
    "    print(\"\\n--- STARTING PREPROCESSING ---\")\n",
    "\n",
    "    print(f\"Loading FULL datasets (Train Rows: {N_TRAIN_ROWS if N_TRAIN_ROWS else 'ALL'})...\")\n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_FILE, nrows=N_TRAIN_ROWS)\n",
    "        df_val = pd.read_csv(VAL_FILE, nrows=N_TEST_ROWS)\n",
    "        df_test = pd.read_csv(TEST_FILE, nrows=N_TEST_ROWS)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Dataset files not found. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Train shape: {df_train.shape}, Val shape: {df_val.shape}, Test shape: {df_test.shape}\")\n",
    "\n",
    "    print(f\"Loading tokenizer: {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def preprocess_function(examples: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "        cc_list = [str(cc) if pd.notna(cc) else \"\" for cc in examples[\"chief_complaint\"]]\n",
    "        hpi_list = [str(hpi) if pd.notna(hpi) else \"\" for hpi in examples[\"history_of_present_illness\"]]\n",
    "\n",
    "        text = [f\"CHIEF COMPLAINT: {cc} | HISTORY: {hpi}\" for cc, hpi in zip(cc_list, hpi_list)]\n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_TOKEN_LENGTH\n",
    "        )\n",
    "\n",
    "        if TARGET_COLUMN in examples:\n",
    "            cleaned_labels = [str(label).strip() if pd.notna(label) else 'Unknown' for label in examples[TARGET_COLUMN]]\n",
    "            tokenized_inputs[\"labels\"] = [label2id.get(label, label2id.get('Unknown', 0)) for label in cleaned_labels]\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "    print(\"Converting to Hugging Face Datasets...\")\n",
    "    ds_train = Dataset.from_pandas(df_train)\n",
    "    ds_val = Dataset.from_pandas(df_val)\n",
    "    ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "    print(\"Tokenizing datasets (this may take a while)...\")\n",
    "    tokenized_train = ds_train.map(preprocess_function, batched=True)\n",
    "    tokenized_val = ds_val.map(preprocess_function, batched=True)\n",
    "    tokenized_test = ds_test.map(preprocess_function, batched=True)\n",
    "\n",
    "    processed_dataset = DatasetDict({\n",
    "        'train': tokenized_train,\n",
    "        'validation': tokenized_val,\n",
    "        'test': tokenized_test\n",
    "    })\n",
    "\n",
    "    # Save to disk\n",
    "    print(f\"Saving processed dataset to {PROCESSED_DATA_DIR}...\")\n",
    "    if os.path.exists(PROCESSED_DATA_DIR):\n",
    "        shutil.rmtree(PROCESSED_DATA_DIR)\n",
    "    processed_dataset.save_to_disk(PROCESSED_DATA_DIR)\n",
    "\n",
    "    print(\"--- PREPROCESSING COMPLETE ---\")\n"
   ],
   "id": "46e87dbf58bf164e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MODEL FINE-TUNING",
   "id": "dab1af413ceb8ff4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:24:11.421651Z",
     "start_time": "2025-11-25T12:24:11.413208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_weighted': f1,\n",
    "        'precision_weighted': precision,\n",
    "        'recall_weighted': recall\n",
    "    }\n",
    "\n",
    "def finetune(label_info: Dict[str, Any], device: torch.device):\n",
    "    print(\"\\n--- STARTING MODEL FINE-TUNING ---\")\n",
    "\n",
    "    try:\n",
    "        processed_dataset = load_from_disk(PROCESSED_DATA_DIR)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No processed data found at {PROCESSED_DATA_DIR}. Run preprocessing first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading pre-trained model: {MODEL_NAME}...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=label_info['num_labels'],\n",
    "        label2id=label_info['label2id'],\n",
    "        id2label=label_info['id2label']\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"Configuring Best Performance Training Arguments...\")\n",
    "    \n",
    "    if os.path.exists(MODEL_OUTPUT_DIR):\n",
    "        print(f\"Cleaning up old model directory {MODEL_OUTPUT_DIR} to free disk space...\")\n",
    "        try:\n",
    "            shutil.rmtree(MODEL_OUTPUT_DIR)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not fully delete old directory: {e}\")\n",
    "            \n",
    "    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE, \n",
    "        per_device_eval_batch_size=BATCH_SIZE, \n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        \n",
    "        fp16=torch.cuda.is_available(),  # Mixed Precision (Major Speedup)\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        \n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,     \n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,         \n",
    "        \n",
    "        save_total_limit=1,              \n",
    "        \n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_dataset['train'],\n",
    "        eval_dataset=processed_dataset['validation'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"Evaluating best model on validation set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(json.dumps(eval_results, indent=2))\n",
    "\n",
    "    print(f\"Saving final model to {MODEL_OUTPUT_DIR}...\")\n",
    "    trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "    \n",
    "    # Save tokenizer with the model so it is self-contained\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "\n",
    "    print(\"--- FINE-TUNING COMPLETE ---\")"
   ],
   "id": "980f915f4f6004c6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EVALUATION ",
   "id": "31587fc742d917cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:24:11.432691Z",
     "start_time": "2025-11-25T12:24:11.427045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_and_infer(label_info: Dict[str, Any], device: torch.device):\n",
    "    print(\"\\n--- STARTING EVALUATION ---\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "        model.to(device)\n",
    "    except OSError:\n",
    "        print(f\"Error: Could not load model from {MODEL_OUTPUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    processed_dataset = load_from_disk(PROCESSED_DATA_DIR)\n",
    "    test_dataset = processed_dataset['test']\n",
    "    \n",
    "    id2label = {int(k): v for k, v in label_info['id2label'].items()}\n",
    "    all_label_indices = list(range(label_info['num_labels']))\n",
    "    all_label_names = [id2label.get(i, f\"UNK_{i}\") for i in all_label_indices] \n",
    "\n",
    "    print(\"Generating predictions on Test Set...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(output_dir=\"./temp_eval\", per_device_eval_batch_size=BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = predictions.label_ids\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_true, y_pred, labels=all_label_indices, target_names=all_label_names, zero_division=0))\n",
    "\n",
    "    print(\"Generating Confusion Matrix...\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=all_label_indices)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=all_label_names, yticklabels=all_label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Triage Confusion Matrix')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(CONFUSION_MATRIX_FILE) or '.', exist_ok=True)\n",
    "    plt.savefig(CONFUSION_MATRIX_FILE)\n",
    "    print(f\"Saved plot to {CONFUSION_MATRIX_FILE}\")\n"
   ],
   "id": "ca88c1d4a66257d2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MAIN EXECUTION",
   "id": "3d7ffb9d24a4efe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    print(\"====== STARTING CLINICAL TRIAGE PIPELINE ======\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    try:\n",
    "        label_info = get_label_maps()\n",
    "        preprocess(label_info['label2id'])\n",
    "        finetune(label_info, device)\n",
    "        evaluate_and_infer(label_info, device)\n",
    "        \n",
    "        print(\" ZIPPING MODEL FOR DOWNLOAD \")\n",
    "        \n",
    "        # This creates 'triage_classifier_model.zip' from the directory\n",
    "        shutil.make_archive(ZIP_NAME, 'zip', MODEL_OUTPUT_DIR)\n",
    "        \n",
    "        print(f\"SUCCESS! The model has been saved and zipped.\")\n",
    "        print(f\"You should see '{ZIP_NAME}.zip' in your file browser now.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "c124e019a44f9bb0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
