{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Hugging Face and Transformers\n",
    "try:\n",
    "    from datasets import Dataset, DatasetDict, load_from_disk\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "        EarlyStoppingCallback,\n",
    "        pipeline\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"ImportError: Hugging Face libraries not found.\")\n",
    "    print(\"Please run: pip install torch transformers datasets scikit-learn pandas numpy matplotlib seaborn accelerate\")\n",
    "    exit()\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- CONFIGURATION ---\n",
    "# Data files (Ensure these paths are correct in your environment)\n",
    "TRAIN_FILE = \"../../../data/bigdata/train.csv\"\n",
    "VAL_FILE = \"../../../data/bigdata/val.csv\"\n",
    "TEST_FILE = \"../../../data/bigdata/test.csv\"\n",
    "\n",
    "# Lighter data settings\n",
    "N_TRAIN_ROWS = 5000  \n",
    "N_TEST_ROWS = 500    \n",
    "\n",
    "# Preprocessing\n",
    "TARGET_COLUMN = 'discharge_disposition'\n",
    "# Using ClinicalBERT for its domain-specific knowledge\n",
    "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "MAX_TOKEN_LENGTH = 512\n",
    "PROCESSED_DATA_DIR = \"../../../data/processed_data/tpaul/processed_triage_data\"\n",
    "LABEL_INFO_FILE = \"../../../data/info/tpaul/label_info.json\"\n",
    "\n",
    "# Training\n",
    "MODEL_OUTPUT_DIR = \"../../../models/results/tpaul/triage_classifier_model\"\n",
    "# --- Lighter training settings ---\n",
    "BATCH_SIZE = 2 \n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Evaluation\n",
    "CONFUSION_MATRIX_FILE = \"../../../evaluation/tpaul/confusion_matrix.png\""
   ],
   "id": "79792063e793c00c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- PREPROCESSING FUNCTIONS ---\n",
    "def get_label_maps() -> Dict[str, Any]:\n",
    "    print(\"Creating complete label mappings from a sample of data...\")\n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_FILE, usecols=[TARGET_COLUMN], nrows=N_TRAIN_ROWS)\n",
    "        df_val = pd.read_csv(VAL_FILE, usecols=[TARGET_COLUMN], nrows=N_TEST_ROWS)\n",
    "        df_test = pd.read_csv(TEST_FILE, usecols=[TARGET_COLUMN], nrows=N_TEST_ROWS)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Make sure {TRAIN_FILE}, {VAL_FILE}, and {TEST_FILE} exist.\")\n",
    "        raise\n",
    "        \n",
    "    all_labels = pd.concat([df_train, df_val, df_test])\n",
    "    all_labels[TARGET_COLUMN] = all_labels[TARGET_COLUMN].fillna('Unknown').astype(str).str.strip()\n",
    "\n",
    "    unique_labels = all_labels[TARGET_COLUMN].unique()\n",
    "    unique_labels.sort()\n",
    "\n",
    "    # Create mappings\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    num_labels = len(unique_labels)\n",
    "\n",
    "    print(f\"Found {num_labels} unique labels in the sample.\")\n",
    "\n",
    "    label_info = {'label2id': label2id, 'id2label': id2label, 'num_labels': num_labels}\n",
    "\n",
    "    # Ensure parent directories exist before saving\n",
    "    os.makedirs(os.path.dirname(LABEL_INFO_FILE) or '.', exist_ok=True)\n",
    "    \n",
    "    # Save mappings to disk\n",
    "    with open(LABEL_INFO_FILE, 'w') as f:\n",
    "        json.dump(label_info, f)\n",
    "\n",
    "    return label_info\n",
    "\n",
    "def preprocess(label2id: Dict[str, int]):\n",
    "    print(\"\\n--- STARTING PREPROCESSING ---\")\n",
    "\n",
    "    # Load Data\n",
    "    print(f\"Loading datasets (Train: {N_TRAIN_ROWS} rows, Val/Test: {N_TEST_ROWS} rows)...\")\n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_FILE, nrows=N_TRAIN_ROWS)\n",
    "        df_val = pd.read_csv(VAL_FILE, nrows=N_TEST_ROWS)\n",
    "        df_test = pd.read_csv(TEST_FILE, nrows=N_TEST_ROWS)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"Please ensure {TRAIN_FILE}, {VAL_FILE}, and {TEST_FILE} are in the correct paths.\")\n",
    "        raise\n",
    "\n",
    "    print(f\"Train shape: {df_train.shape}, Val shape: {df_val.shape}, Test shape: {df_test.shape}\")\n",
    "\n",
    "    print(f\"Loading tokenizer: {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def preprocess_function(examples: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "        cc_list = [str(cc) if cc is not None and pd.notna(cc) else \"\" for cc in examples[\"chief_complaint\"]]\n",
    "        hpi_list = [str(hpi) if hpi is not None and pd.notna(hpi) else \"\" for hpi in examples[\"history_of_present_illness\"]]\n",
    "\n",
    "        # Text concatenation for model input\n",
    "        text = [f\"CHIEF COMPLAINT: {cc} | HISTORY: {hpi}\" for cc, hpi in zip(cc_list, hpi_list)]\n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_TOKEN_LENGTH\n",
    "        )\n",
    "\n",
    "        if TARGET_COLUMN in examples:\n",
    "            cleaned_labels = [str(label).strip() if label is not None and pd.notna(label) else 'Unknown' for label in examples[TARGET_COLUMN]]\n",
    "            tokenized_inputs[\"labels\"] = [label2id.get(label, label2id['Unknown']) for label in cleaned_labels]\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "    # Convert Pandas to Hugging Face Datasets\n",
    "    print(\"Converting Pandas DataFrames to Hugging Face Datasets...\")\n",
    "    ds_train = Dataset.from_pandas(df_train)\n",
    "    ds_val = Dataset.from_pandas(df_val)\n",
    "    ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    tokenized_train = ds_train.map(preprocess_function, batched=True)\n",
    "    tokenized_val = ds_val.map(preprocess_function, batched=True)\n",
    "    tokenized_test = ds_test.map(preprocess_function, batched=True)\n",
    "\n",
    "    # Create the final DatasetDict\n",
    "    processed_dataset = DatasetDict({\n",
    "        'train': tokenized_train,\n",
    "        'validation': tokenized_val,\n",
    "        'test': tokenized_test\n",
    "    })\n",
    "    print(\"\\nTokenization complete. Processed dataset:\")\n",
    "    print(processed_dataset)\n",
    "\n",
    "    # Save Processed Data\n",
    "    print(f\"Saving processed dataset to disk at {PROCESSED_DATA_DIR}...\")\n",
    "    if os.path.exists(PROCESSED_DATA_DIR):\n",
    "        print(f\"Removing old processed data at {PROCESSED_DATA_DIR}\")\n",
    "        shutil.rmtree(PROCESSED_DATA_DIR)\n",
    "    processed_dataset.save_to_disk(PROCESSED_DATA_DIR)\n",
    "\n",
    "    print(\"--- PREPROCESSING COMPLETE ---\")\n"
   ],
   "id": "2641bdeaef7129b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- MODEL FINE-TUNING FUNCTIONS ---\n",
    "def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_weighted': f1,\n",
    "        'precision_weighted': precision,\n",
    "        'recall_weighted': recall\n",
    "    }\n",
    "\n",
    "def finetune(label_info: Dict[str, Any], device: torch.device):\n",
    "    \"\"\"\n",
    "    Loads the processed data, fine-tunes the ClinicalBERT model,\n",
    "    and saves the best model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- STARTING MODEL FINE-TUNING ---\")\n",
    "\n",
    "    # Load Processed Data\n",
    "    try:\n",
    "        print(f\"Loading processed dataset from {PROCESSED_DATA_DIR}...\")\n",
    "        processed_dataset = load_from_disk(PROCESSED_DATA_DIR)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Processed data not found at {PROCESSED_DATA_DIR}.\")\n",
    "        raise\n",
    "\n",
    "    # Load Pre-trained Model\n",
    "    print(f\"Loading pre-trained model: {MODEL_NAME}...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=label_info['num_labels'],\n",
    "        label2id=label_info['label2id'],\n",
    "        id2label=label_info['id2label']\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"Defining training arguments...\")\n",
    "    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=MODEL_OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE, \n",
    "        per_device_eval_batch_size=BATCH_SIZE, \n",
    "        gradient_accumulation_steps=4, \n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,      \n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\"              \n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_dataset['train'],\n",
    "        eval_dataset=processed_dataset['validation'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate and Save\n",
    "    print(\"Evaluating best model on validation set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(json.dumps(eval_results, indent=2))\n",
    "\n",
    "    print(f\"Saving final model to {MODEL_OUTPUT_DIR}\")\n",
    "    trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "\n",
    "    print(\"--- FINE-TUNING COMPLETE ---\")"
   ],
   "id": "9171a2c222c20d04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- EVALUATION FUNCTIONS ---\n",
    "def evaluate_and_infer(label_info: Dict[str, Any], device: torch.device):\n",
    "    print(\"\\n--- STARTING EVALUATION & INFERENCE ---\")\n",
    "\n",
    "    print(f\"Loading fine-tuned model from {MODEL_OUTPUT_DIR}...\")\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "        model.to(device)\n",
    "    except OSError:\n",
    "        print(f\"Error: Model not found at {MODEL_OUTPUT_DIR}.\")\n",
    "        raise\n",
    "\n",
    "    print(f\"Loading processed test data from {PROCESSED_DATA_DIR}...\")\n",
    "    try:\n",
    "        processed_dataset = load_from_disk(PROCESSED_DATA_DIR)\n",
    "        test_dataset = processed_dataset['test']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Processed data not found at {PROCESSED_DATA_DIR}.\")\n",
    "        raise\n",
    "\n",
    "    id2label = {int(k): v for k, v in label_info['id2label'].items()}\n",
    "    all_label_indices = list(range(label_info['num_labels']))\n",
    "    all_label_names = [id2label.get(i, f\"UNK_{i}\") for i in all_label_indices] \n",
    "\n",
    "    print(\"Running predictions on the test set...\")\n",
    "    os.makedirs(\"./temp_eval\", exist_ok=True)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(output_dir=\"./temp_eval\", per_device_eval_batch_size=BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = predictions.label_ids\n",
    "\n",
    "    print(\"\\n--- Test Set Classification Report ---\")\n",
    "\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=all_label_indices, \n",
    "        target_names=all_label_names,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    print(report)\n",
    "\n",
    "    print(\"Generating confusion matrix...\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=all_label_indices)\n",
    "    \n",
    "    # Calculate figure size dynamically\n",
    "    label_count = label_info['num_labels']\n",
    "    figsize_x = max(10, label_count * 0.8)\n",
    "    figsize_y = max(8, label_count * 0.6)\n",
    "\n",
    "    plt.figure(figsize=(figsize_x, figsize_y))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=all_label_names, yticklabels=all_label_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix for Triage Prediction (Test Set)')\n",
    "    plt.tight_layout() \n",
    "\n",
    "    os.makedirs(os.path.dirname(CONFUSION_MATRIX_FILE) or '.', exist_ok=True)\n",
    "    plt.savefig(CONFUSION_MATRIX_FILE)\n",
    "    print(f\"Confusion matrix saved to {CONFUSION_MATRIX_FILE}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n--- Running Sample Inference ---\")\n",
    "\n",
    "    triage_pipe = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if (device.type == \"cuda\") else -1 \n",
    "    )\n",
    "\n",
    "    # Test with a new patient\n",
    "    new_chief_complaint = \"Patient presents with chest pain and difficulty breathing.\"\n",
    "    new_history = \"73-year-old male with a history of hypertension and diabetes. Smoker for 30 years.\"\n",
    "\n",
    "    input_text = f\"CHIEF COMPLAINT: {new_chief_complaint} | HISTORY: {new_history}\"\n",
    "\n",
    "    print(f\"\\nInput Text:\\n{input_text}\")\n",
    "\n",
    "    prediction = triage_pipe(input_text)\n",
    "    print(\"\\n--- Prediction ---\")\n",
    "    print(json.dumps(prediction, indent=2))\n",
    "\n",
    "    all_scores = triage_pipe(input_text, return_all_scores=True)\n",
    "    print(\"\\n--- All scores ---\")\n",
    "    all_scores_sorted = sorted(all_scores[0], key=lambda x: x['score'], reverse=True)\n",
    "    print(json.dumps(all_scores_sorted, indent=2))\n",
    "\n",
    "    print(\"--- EVALUATION COMPLETE ---\")"
   ],
   "id": "9c7f76f5d349c07b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- MAIN EXECUTION ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Runs the full pipeline:\n",
    "    1. Preprocess data\n",
    "    2. Fine-tune model\n",
    "    3. Evaluate and infer\n",
    "    \"\"\"\n",
    "    print(\"====== STARTING TRIAGE MODEL ======\")\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Get label mappings\n",
    "        label_info = get_label_maps()\n",
    "        # Step 2: Preprocess and tokenize data\n",
    "        preprocess(label_info['label2id'])\n",
    "        # Step 3: Fine-tune the model\n",
    "        finetune(label_info, device)\n",
    "        # Step 4: Evaluate and run sample inference\n",
    "        evaluate_and_infer(label_info, device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- FATAL ERROR IN PIPELINE ---\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"====== TRIAGE MODEL COMPLETE ======\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "304a0b35e53f3f22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
