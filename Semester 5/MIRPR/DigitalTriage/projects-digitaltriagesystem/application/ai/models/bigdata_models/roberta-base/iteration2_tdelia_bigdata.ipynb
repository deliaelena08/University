{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lab 3 Implementation",
   "id": "d7e1c162bda9195c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  (TF-IDF + LinearSVC)",
   "id": "3edd986a4a60d4af"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-11-12T12:00:39.000460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "BASE_PATH = 'dataset_split/discharge-notes/'\n",
    "TRAIN_FILE = os.path.join(BASE_PATH, 'train.csv')\n",
    "VAL_FILE = os.path.join(BASE_PATH, 'val.csv')\n",
    "TEST_FILE = os.path.join(BASE_PATH, 'test.csv')\n",
    "\n",
    "\n",
    "print(\"1. Loading data...\")\n",
    "\n",
    "try:\n",
    "    df_train = pd.read_csv(TRAIN_FILE)\n",
    "    df_val = pd.read_csv(VAL_FILE)\n",
    "    df_test = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    df_train_full = pd.concat([df_train, df_val], ignore_index=True)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: File not found: {os.path.abspath(TRAIN_FILE).replace('train.csv', '...')}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Train/Val samples: {len(df_train_full)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "\n",
    "REQUIRED_COLUMNS = ['chief_complaint', 'history_of_present_illness', 'major_surgical_procedure']\n",
    "for col in REQUIRED_COLUMNS:\n",
    "    if col not in df_train_full.columns or col not in df_test.columns:\n",
    "        raise ValueError(f\"ERROR: Missing column '{col}'\")\n",
    "\n",
    "\n",
    "# 2. FEATURE ENGINEERING\n",
    "def create_text_feature(df):\n",
    "    return df['chief_complaint'].fillna('') + \" \" + df['history_of_present_illness'].fillna('')\n",
    "\n",
    "X_train = create_text_feature(df_train_full)\n",
    "X_test = create_text_feature(df_test)\n",
    "\n",
    "y_train_str = df_train_full['major_surgical_procedure'].fillna('UNKNOWN')\n",
    "y_test_str = df_test['major_surgical_procedure'].fillna('UNKNOWN')\n",
    "\n",
    "all_unique_labels = pd.concat([y_train_str, y_test_str]).unique()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(all_unique_labels)\n",
    "\n",
    "y_train = le.transform(y_train_str)\n",
    "y_test = le.transform(y_test_str)\n",
    "\n",
    "target_names = le.classes_\n",
    "print(f\"Number of classes: {len(target_names)}\")\n",
    "print(f\"First 5 classes: {target_names[:5]}...\")\n",
    "\n",
    "print(\"2. Building Pipeline (TF-IDF + LinearSVC)...\")\n",
    "model_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        min_df=5,\n",
    "        max_features=50000\n",
    "    )),\n",
    "    ('clf', LinearSVC(\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )),\n",
    "])\n",
    "\n",
    "print(\"3. Training model...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"4. Evaluating model on Test Set (Sklearn metrics)...\")\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "test_f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\n## Primary Results\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"F1-Macro: {test_f1_macro:.4f}\")\n",
    "print(f\"F1-Weighted: {test_f1_weighted:.4f}\")\n",
    "\n",
    "print(\"\\n## Classification Report\")\n",
    "if len(target_names) > 20:\n",
    "    print(f\"Warning: {len(target_names)} classes. Printing full report.\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n## Confusion Matrix (Shape)\")\n",
    "print(f\"Matrix dimension: {cm.shape[0]} x {cm.shape[1]}\")\n",
    "print(\"Actual matrix should be included in your lab report.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"5. CONCLUSION FOR LAB REPORT\")\n",
    "print(f\"Classical model (TF-IDF + LinearSVC) achieved a Weighted F1-Score of {test_f1_weighted:.4f}.\")\n",
    "print(\"This result serves as a strong **benchmark** for comparison against SOTA Transformer models.\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data...\n",
      "Train/Val samples: 298614\n",
      "Test samples: 33180\n",
      "Number of classes: 122477\n",
      "First 5 classes: ['\"Very mild localized erythema and loss of vascularity without bleeding were noted at 20-30cm. These findings are compatible with very mild colitis versus scope trauma and are far less pronounced than would be expected given the CT report.... Previous end to side ileo-colonic anastomosis of the ascending colon Extremely subtle mucosal changes suggestive of mild colitis in the sigmoid colon Otherwise normal colonoscopy to ileum.... The findings on this exam do not explain the degree of abdominal pain. Suggest continued consultation with the Pain Service to optimize therapy for functional abdominal pain. Consider topical therapy (cortifoam or rowasa) to treat subtle features of colitis.\" EGD with gastritis and retained fluids in the stomach, otherwise WNL to third part of duodenum.'\n",
      " '#  1. Open reduction internal fixation right bicondylar tibial plateau fracture with internal fixation. 2. Open reduction internal fixation left tibial shaft fracture with internal fixation. 3. Closed treatment of bilateral proximal fibula fractures without manipulation # Right IJ placement () # Right IJ removal ()'\n",
      " '#  EGD w/ biopsies by Gastroenterology (Dr.  #  Irrigation of skin down to bone of the left lower extremity wound (20x10cm) by Plastic Surgery (Dr.  #  Below knee amputation by Vascular Surgery(Dr.'\n",
      " '#  guided AV graft thrombectmony on'\n",
      " '#  guided drainage (): Successful CT-guided placement of an  pigtail catheter into the anterior abdominal collection. Samples were sent for microbiology evaluation. #  guided drainage (): Successful CT-guided placement of an  pigtail catheter into the perisplenic collection. Samples were sent for microbiology evaluation. #  guided drainage (): 1. Successful CT-guided placement of an  pigtail catheter into the right pelvic collection. 2. Successful CT-guided aspiration of perihepatic collection (no drain). Samples were sent for microbiology evaluation. # Removal of ant abd, perisplenic drains () # Removal of perihepatic drain  # Downsized tach ()']...\n",
      "2. Building Pipeline (TF-IDF + LinearSVC)...\n",
      "3. Training model...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### RoBerta",
   "id": "ca4507da9e5d2d15"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-13T07:14:22.506148Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tapuc\\Desktop\\University\\Semester 5\\MIRPR\\DigitalTriage\\projects-digitaltriagesystem\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and Pre-processing data...\n",
      "Train/Val samples: 298614\n",
      "Test samples: 33180\n",
      "Filtering labels: Keeping Top 100 classes + 'OTHER_PROCEDURE'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5FB82F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 28df95fd-ecc2-45b1-a803-f2541ba396d2)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Număr final de clase: 101\n",
      "\n",
      "2. Încărcare Model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F68F50>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: feaf1840-3498-4b10-b742-1d88d7a58da0)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F69590>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b8462544-d53e-4d36-bc3a-87a1cea45c0b)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F69950>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d631f686-1e49-4b2e-a9c0-143cc3513c5a)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F69D10>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: a3650148-6021-4f68-9e90-e5ce8e930dee)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F6A0D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3ea67339-3e19-4c69-9408-b08d1f5822e4)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280AA2BB890>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: fc767934-ef31-4f15-9b86-08d31d73dc27)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F69950>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b3278dca-1051-42d7-872e-401bb5c83203)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F69450>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 1a177cac-c47a-4d3b-929a-1186133593a0)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F696D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: c09b76c8-ebae-4800-8618-1e95c9a54f77)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F68CD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3c1bbb00-c2db-4d58-bc9c-6e340748f427)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /roberta-base/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000280C5F68A50>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: a0d0033d-4078-49e3-88f5-5fdf46e826a0)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Conversie la formatul Hugging Face Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 298614/298614 [00:56<00:00, 5248.82 examples/s]\n",
      "Map: 100%|██████████| 33180/33180 [00:06<00:00, 5142.91 examples/s]\n",
      "C:\\Users\\tapuc\\AppData\\Local\\Temp\\ipykernel_32928\\1386925065.py:181: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n",
      "C:\\Users\\tapuc\\Desktop\\University\\Semester 5\\MIRPR\\DigitalTriage\\projects-digitaltriagesystem\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling set de antrenare la 50000 mostre din 298614.\n",
      "\n",
      "4. Începe Fine-Tuning cu RoBERTa-Base (1 Epocă, 50k mostre)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 77/782 1:43:31 < 16:13:08, 0.01 it/s, Epoch 0.10/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "BASE_PATH = 'dataset_split/discharge-notes/'\n",
    "TRAIN_FILE = os.path.join(BASE_PATH, 'train.csv')\n",
    "VAL_FILE = os.path.join(BASE_PATH, 'val.csv')\n",
    "TEST_FILE = os.path.join(BASE_PATH, 'test.csv')\n",
    "\n",
    "\n",
    "print(\"1. Loading and Pre-processing data...\")\n",
    "\n",
    "try:\n",
    "    df_train = pd.read_csv(TRAIN_FILE)\n",
    "    df_val = pd.read_csv(VAL_FILE)\n",
    "    df_test = pd.read_csv(TEST_FILE)\n",
    "    df_train_full = pd.concat([df_train, df_val], ignore_index=True)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: File not found: {os.path.abspath(TRAIN_FILE).replace('train.csv', '...')}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Train/Val samples: {len(df_train_full)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "\n",
    "REQUIRED_COLUMNS = ['chief_complaint', 'history_of_present_illness', 'major_surgical_procedure']\n",
    "for col in REQUIRED_COLUMNS:\n",
    "    if col not in df_train_full.columns or col not in df_test.columns:\n",
    "        raise ValueError(f\"ERROR: Missing column '{col}'\")\n",
    "\n",
    "def create_text_feature(df):\n",
    "    return df['chief_complaint'].fillna('') + \" \" + df['history_of_present_illness'].fillna('')\n",
    "\n",
    "X_train = create_text_feature(df_train_full)\n",
    "X_test = create_text_feature(df_test)\n",
    "\n",
    "y_train_str = df_train_full['major_surgical_procedure'].fillna('UNKNOWN')\n",
    "y_test_str = df_test['major_surgical_procedure'].fillna('UNKNOWN')\n",
    "\n",
    "TOP_K = 100\n",
    "print(f\"Filtering labels: Keeping Top {TOP_K} classes + 'OTHER_PROCEDURE'\")\n",
    "\n",
    "def clean_label(label):\n",
    "    return ''.join(c.lower() if c.isalnum() or c.isspace() else '' for c in label).strip()\n",
    "\n",
    "y_train_str_cleaned = y_train_str.apply(clean_label)\n",
    "y_test_str_cleaned = y_test_str.apply(clean_label)\n",
    "\n",
    "label_counts = y_train_str_cleaned.value_counts()\n",
    "top_labels = label_counts.head(TOP_K).index.tolist()\n",
    "\n",
    "def filter_labels(label, top_labels_list):\n",
    "    if label in top_labels_list:\n",
    "        return label\n",
    "    return 'otherprocedure'\n",
    "\n",
    "y_train_str_filtered = y_train_str_cleaned.apply(lambda x: filter_labels(x, top_labels))\n",
    "y_test_str_filtered = y_test_str_cleaned.apply(lambda x: filter_labels(x, top_labels))\n",
    "\n",
    "all_unique_labels_filtered = pd.concat([y_train_str_filtered, y_test_str_filtered]).unique()\n",
    "le = LabelEncoder()\n",
    "le.fit(all_unique_labels_filtered)\n",
    "\n",
    "y_train = le.transform(y_train_str_filtered)\n",
    "y_test = le.transform(y_test_str_filtered)\n",
    "\n",
    "target_names = le.classes_\n",
    "num_labels = len(target_names)\n",
    "print(f\"Numar final de clase: {num_labels}\")\n",
    "\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = {cls: w for cls, w in zip(np.unique(y_train), class_weights_array)}\n",
    "class_weights_tensor = torch.from_numpy(class_weights_array.astype(np.float32))\n",
    "\n",
    "\n",
    "MODEL_CHECKPOINT = 'roberta-base'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "\n",
    "print(f\"\\n2. Incarcare Model: {MODEL_CHECKPOINT}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "print(\"3. Conversie la formatul Hugging Face Dataset...\")\n",
    "\n",
    "train_data = pd.DataFrame({'text': X_train.tolist(), 'label': y_train.tolist()})\n",
    "test_data = pd.DataFrame({'text': X_test.tolist(), 'label': y_test.tolist()})\n",
    "\n",
    "raw_train_dataset = Dataset.from_pandas(train_data)\n",
    "raw_test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "tokenized_train_dataset = raw_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = raw_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "SAMPLE_SIZE = 50000\n",
    "if len(tokenized_train_dataset) > SAMPLE_SIZE:\n",
    "    print(f\"Sampling set de antrenare la {SAMPLE_SIZE} mostre din {len(tokenized_train_dataset)}.\")\n",
    "    indices = np.random.choice(len(tokenized_train_dataset), SAMPLE_SIZE, replace=False)\n",
    "    tokenized_train_dataset = tokenized_train_dataset.select(indices)\n",
    "else:\n",
    "    print(\"Nu se aplica sampling.\")\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\"])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\"])\n",
    "\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "   def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1_w = f1_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"f1_weighted\": f1_w}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Roberta_results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./Roberta_logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n4. Icepe Fine-Tuning cu RoBERTa-Base (1 Epoca, 50k mostre)...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n5. Evaluare Finala (Pe Test Set):\")\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "print(\"\\nAntrenare terminata. Metricele sunt disponibile în 'Roberta_logs'.\")"
   ],
   "id": "966d449f685f2aec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea3dbac2ff542797"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
