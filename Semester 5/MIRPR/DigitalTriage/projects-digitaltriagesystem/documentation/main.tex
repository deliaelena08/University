\documentclass{article}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{hyperref}
\setlist[itemize]{label=\textbullet, leftmargin=*}
\setlist[enumerate]{leftmargin=*}

\title{Digital Triage System}
\author{
    Tapuc Delia \\
    Titieni Paul \\
    Stiube Denis \\
    Costan Alex
}
\date{}

\begin{document}

\maketitle

\section{Problem & Solution Statement}

In Romanian hospitals, the medical triage process is largely manual, leading to long wait times, errors in urgency classification, and high pressure on medical staff. The primary users are public and private hospitals facing overcrowding in emergency rooms.

The proposed solution, \textbf{Digital Triage System (DTS)}, is an intelligent web application that automates patient triage by collecting their data (symptoms, vital signs, medical history) and classifying the urgency level using AI algorithms and standardized medical scoring systems. The system allows medical staff to view prioritized patients in real-time, reduce human errors, and improve hospital workflow efficiency.

\section*{Core Features} 
\begin{enumerate} 
    \item Digital patient triage form – automatic collection of symptoms, vital signs, and medical history.
    \item AI algorithm for urgency classification – using predictive models and/or existing triage recommendation libraries, allowing the estimation of the patient's urgency level based on collected data.
    \item Medical dashboard – displays the list of patients in priority order, with real-time updates. 
    \item Authentication and role management – secure access for doctors, nurses, and administrative staff.
    \item Integration with hospital systems (HIS/EMR) – synchronization with existing medical records. 
\end{enumerate}

\section*{Technologies Used}

\begin{itemize} 
\item \textbf{Backend:} Python (FastAPI) – for the API and AI triage logic.
\item \textbf{Frontend:} React + TypeScript – for a responsive web interface, accessible from any device. 
\item \textbf{Database:} PostgreSQL – for secure storage of patient data and triage history.
\item \textbf{AI/ML Layer:} Predictive models and existing recommendation libraries – for estimating the patient's urgency level based on collected data. 
\end{itemize}

\section*{The Problem Solved by AI}

\textbf{What is solved:} The manual medical triage process, which generates long wait times and errors in urgency classification.

\textbf{Why it is important:} Reducing wait times and increasing triage accuracy can save lives, reduce stress on medical staff, and streamline hospital workflows.

\textbf{Who are the users:} Medical staff (doctors, nurses) in public and private hospitals, administrative staff, and patients.

\textbf{Input and output data:}
\begin{itemize}
    \item \textbf{Input:} patient symptoms, vital signs, medical history, other relevant digitally collected data.
    \item \textbf{Output:} patient urgency level, displayed on a prioritized dashboard for medical staff.
\end{itemize}

\textbf{How performance is measured:} \begin{itemize} 
    \item Reduction in patient wait times.
    \item Accuracy of urgency classification compared to manual triage. 
    \item User feedback and the number of identified medical errors. 
\end{itemize}


\section{Related Work and Useful Tools}

In this section, we present relevant work and tools in the area of AI-powered digital triage and remote health monitoring. The following two systems are included as initial references; additional contributions will be added by team members.

% Tapuc Delia

\subsection{Shen AI}
\textbf{Reference:} \href{https://shen.ai/blog/ai-powered-triage-real-time-health-monitoring#the-complementary-power-of-ai-triage-and-remote-health-monitoring}{AI-Powered Triage and Real-Time Health Monitoring}


\textbf{Data:} Shen AI's technology is trained on a dataset of over 7 million data points from a diverse user base, including approximately 400,000 individuals.

\textbf{Algorithm:}
Shen AI uses two main technologies to measure over 30 health indicators through a 30-second facial scan:
\begin{itemize}
    \item \textbf{Remote Photoplethysmography (rPPG):} Analyzes subtle skin color changes caused by blood flow, enabling measurement of heart rate, respiratory rate, and other vital signs.
    \item \textbf{Remote Ballistocardiography (rBCG):} Detects microscopic facial movements generated by the heart's mechanical activity, estimating heart rate and cardiac output.
\end{itemize}
These technologies are integrated into a \textit{Multimodal Sensor Engine} that analyzes multiple light wavelengths to ensure accurate vital sign detection regardless of skin tone or lighting conditions.

\textbf{Performance:} Clinically validated studies, including one by Wroclaw Medical University, demonstrated that Shen AI is as accurate as contact-based devices such as blood pressure monitors and ECGs for heart rate, respiratory rate, and heart rate variability (HRV).

\textbf{Libraries/Technologies:}
\begin{itemize}
    \item \textbf{Shen.AI SDK:} Provides integration examples for all supported platforms, including native compiled code for Android/iOS, WebAssembly for web, and real-time computer vision and neural network algorithms. Includes built-in user interface for measurement guidance and results display. (\url{https://github.com/mxlaboratories/shenai-sdk?utm_source=chatgpt.com})
    \item \textbf{Multi-Tonal Sensing Technology:} Ensures accurate measurements under low light and across diverse skin tones.
    \item \textbf{On-device Real-time Video Processing:} All processing occurs locally on the user's device, ensuring privacy and GDPR compliance.
\end{itemize}

\subsection{ADA}
\textbf{Reference:} \href{https://about.ada.com/improving-patient-pathways-with-ada-digital-triage/}{Improving Patient Pathways with Ada Digital Triage}

\textbf{Data:} Initial clinical data for training were derived from medical literature, manuals, clinical guidelines, and case reports. The system also incorporates user-provided data to inform symptom and medical history information. Validation involved published clinical studies such as BMJ Open and Annals of Surgery.

\textbf{Algorithm:}
Ada uses a probabilistic reasoning model (Bayesian reasoning):
\begin{itemize}
    \item Determines the most probable diagnosis based on reported symptoms.
    \item Updates probabilities dynamically as users respond to questions, aiming to minimize the number of questions while maximizing diagnostic accuracy.
\end{itemize}

\textbf{Knowledge Base:}
\begin{itemize}
    \item Built and reviewed by medical professionals.
    \item Incorporates scientific literature, clinical guidelines, epidemiology, disease models, and case reports.
    \item Combines medical knowledge with user data to generate personalized triage recommendations.
\end{itemize}

\textbf{Digital Triage Engine:}
\begin{itemize}
    \item Determines urgency levels: self-care at home, doctor consultation, or immediate emergency.
    \item Provides a list of possible causes for symptoms, ranked by probability.
\end{itemize}

\textbf{Performance:} A study in \textit{Annals of Surgery} showed that using Ada alongside emergency department physicians significantly improved diagnostic accuracy (87.3\%) compared to physicians alone (80.9\%), reducing complications and hospitalization time.

\textbf{Libraries/Technologies:} Ada Health leverages a probabilistic reasoning model supported by a comprehensive medical knowledge base, covering common and rare diseases and integrating literature, manuals, and regional epidemiology.

% Additional articles can be added here by team members.
% Titieni Paul

\subsection{Learning medical triage from clinicians using Deep Q-Learning}
\textbf{Reference:} \href{https://arxiv.org/abs/2003.12828}{Learning medical triage from clinicians using Deep Q-Learning (arXiv)}

\textbf{Data:} 1,374 clinical vignettes created by physicians, with each vignette associated on average with 3.8 triage decisions made by experts (physicians).

\textbf{Technologies applied:} Reinforcement Learning, specifically a variant of Deep Q‑Learning. The agent was trained to decide when to stop asking questions and make the triage decision, instead of following a rigid decision tree.

\textbf{Performance:} The system produced “safe” triage decisions in $\sim$94\% of cases, and agreed with expert medical decisions in $\sim$85\% of cases.

\textbf{Observations:} The study demonstrates the potential of RL to automatically adjust triage workflows, but the data set is relatively small (vignettes), and it is unclear how much it has been validated in real emergency scenarios.

\subsection{Development and Comparative Evaluation of Three Artificial Intelligence Models for Predicting Triage in Emergency Departments}
\textbf{Data:} Retrospective data from a French hospital (adult triage data, 7 months) at Roger Salengro Hospital in Lille. The study compared three AI models: a classical NLP model ('TRIAGEMASTER'), an LLM model ('URGENTIAPARSE') and a JEPA model ('EMERGINET').

\textbf{Technologies applied:}
\begin{itemize}
    \item \textbf{ Classical NLP model:} Probabilistic language processing for transcripts / triage forms.
    \item \textbf{LLM model:} Large language models to interpret patient-related information.
    \item \textbf{JEPA (Joint Embedding Predictive Architecture):} An emergent multi-modal embedding architecture for triage prediction.
\end{itemize}

\textbf{Performance:} The best model was URGENTIAPARSE (LLM), achieving F1 = 0.900 and AUC-ROC = 0.879 for triage prediction, outperforming the other two models and the baseline evaluation of nurse triage.

\textbf{Observations:} The study suggests that LLM-based models can deliver high performance in triage prediction, but caution is needed for practical adoption (ethics, transparency, extensive validation).

% Costan Alex

\subsection{Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews}
\textbf{Reference:} \href{https://arxiv.org/pdf/2504.11977}{Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews (PDF)}\\

\textbf{Data:} A real-world dataset of triage interviews from Triage24, a
questionnaire-based digital triage system, developed by Platform24 wich contains over 330000 complete triage interviews with definite outcomes (each interview contains about 12 questions, which means that in total are over 4 million answers).\\

\textbf{Dataset:} The dataset created contains over 4000 columns, and in every row only 10-20 columns are empty. They created 3 main datasets: \textbf{Experimental Dataset} (15,000 interviews and over 2,000 features for model
selection and testing), \textbf{Training Dataset} (has 80\% of colected data), \textbf{Test Dataset} (has 20\% of colected data). The interview records are only from adult patients.\\

\textbf{Model selection:} Traditional models have limited performance on sparse and complex questionnaire-based data, which is why tree-based models were chosen for their ability to handle complexity and sparsity, while TabTransformer is used as a benchmark due to its capability to transform categorical features into robust contextual embeddings and withstand missing or noisy data.\\

\textbf{Used Models:} HistGradientBoostingClassifier, RandomForestClassifier, XGBClassifier, LGBMClassifier, and CatBoostClassifier for handling data complexity and sparsity; TabTransformer as a benchmark for evaluating performance on complex, categorical data.\\

They conducted numerous experiments, including training the models on incomplete questionnaires (they created incomplete questionnaires by progressively removing 20\% of the responses, resulting in questionnaires with different levels of completeness: 100\%, 80\%, 60\%, and 40\%). \\

\textbf{Results:} The obtained results were as follows (the best ones):
\begin{itemize}
    \item LGBMClassifier: Achieved an accuracy of 88.2\% for complete questionnaires.
    \item CatBoostClassifier: Achieved an accuracy of 86.4\% for complete questionnaires.
    \item TabTransformer: Demonstrated an accuracy of over 80\% across all levels of completeness, but required significantly longer training time, indicating the need for more powerful computational resources.
\end{itemize}
These results highlight a linear correlation between the level of questionnaire completeness and prediction accuracy:
\begin{itemize}
    \item 80\% completeness: Accuracy of 79.6\%
    \item 60\% completeness: Accuracy of 58.9\%
    \item 40\% completeness: Accuracy of 45.7\%
\end{itemize}

\subsection{Artificial Intelligence Decision Support for Medical Triage}
\textbf{Reference:} \href{https://arxiv.org/pdf/2011.04548}{Artificial Intelligence Decision Support for Medical Triage}\\

\textbf{Data:} The dataset was created using more than 900k case records written in German and collected over more than 7 years. The records are notes that call centers agents and doctors took while talking to the patients over the phone. They are structured in sections, contains a subjective description of the patinet's problem.\\

\textbf{Used models and techniques:} The uses \textbf{NLP (natural language processing)} to extract and analyze the medical entities from text, \textbf{text preprocessing} to correct errors and normalize data, \textbf{NER (Named Entity Recognition)} to identify symptoms and diagnoses, \textbf{concept clustering and dynamic ontologies} to group entities, and \textbf{deep learning models (CNN, Bi-GRU, Bi-LSTM with attention)}.\\

\textbf{Question generation AI module:} An essential part of the triage process is the question-and-answer session, during which the patient is asked additional relevant symptoms. Since patients may not have the knowledge to complete the questionnaires accurately, the question generation algorithm dynamically determines which medical concepts should be addressed, emulating a human expert’s decision. In the described system, question generation is performed either using information retrieval techniques (query expansion, entropy, mutual information) or neural networks, which are trained to predict masked concepts from patient cases. This step optimizes the collection of additional information, improving patient risk classification and prioritization. \\

\textbf{Results:} Approach 1 uses a knowledge graph (KG) to find similar patients based on node and edge weights. No numerical accuracy is reported, but the method is extremely fast, with response times under 4 seconds, and supports scalability, traceability, and transparency in recommendations.

Approach 2 combines the KG with embeddings and CNN/ML models to classify patient risk. Reported results show f-scores of 87.5\% for high-risk, 74\% for medium-risk, and 90.4\% for low-risk patients. This method is deep learning-based and considered a “black box,” requiring additional explanation methods for its predictions.

% Stiube Denis

\subsection{The role of AI in emergency department triage: An integrative systematic review}
\textbf{Reference:} \href{https://pubmed.ncbi.nlm.nih.gov/40306071/}{The role of AI in emergency department triage: An integrative systematic review}\\

\textbf{Data:} The studies analyzed in this systematic review rely on real-world clinical data collected from emergency departments (EDs), primarily extracted from electronic health records (EHRs) and triage systems. The datasets encompass diverse patient populations across multiple sites and healthcare institutions.\\

Data types used:
\begin{itemize}
    \item \textbf{Vital signs} (e.g., blood pressure, heart rate, temperature, oxygen saturation)
    \item \textbf{Demographic} information (e.g., age, sex)
    \item \textbf{Mode of arrival} (e.g., ambulance, walk-in)
    \item \textbf{Disease-specific clinical markers} relevant to acute conditions
    \item \textbf{Free-text clinical notes, processed using Natural Language Processing (NLP)} to extract medical concepts and contextual features
\end{itemize}

\textbf{Methods:} Following PRISMA 2020 guidelines, we systematically searched PubMed, CINAHL, Scopus, Web of Science, and IEEE Xplore for studies on AI/ML-driven ED triage published through January 2025. Two independent reviewers screened studies, extracted data, and assessed quality using PROBAST, with findings synthesized thematically.\\

\textbf{Results:} Twenty-six studies met inclusion criteria. ML-based triage models consistently outperformed traditional tools, often achieving AUCs $>$ 0.80 for high acuity outcomes (e.g., hospital admission, ICU transfer). Key predictors included vital signs, age, arrival mode, and disease-specific markers. Incorporating free-text data via natural language processing enhances accuracy and sensitivity. Advanced ML techniques, such as gradient boosting and random forests, generally surpassed simpler models across diverse populations. Reported benefits included reduced ED overcrowding, improved resource allocation, fewer mis-triaged patients, and potential patient outcome improvements.\\

\subsection{Using machine learning and natural language processing in triage for prediction of clinical disposition in the emergency department}
\textbf{Reference:} \href{https://bmcemergmed.biomedcentral.com/articles/10.1186/s12873-024-01152-1/}{Using machine learning and natural language processing in triage for prediction of clinical disposition in the emergency department}\\

\textbf{Data collection and processing:} Both structured and unstructured information recorded by triage nurses were stored in electronic medical records (EMRs). Structured information on age, sex, body mass index, vital signs, consciousness level, use and type of indwelling tube (e.g., central venous catheter, endotracheal tube, tracheostomy tube, arterial catheter, nasopharyngeal tube, Foley catheter, and drainage tube), whether the patient was transferred and from which facility, mode of arrival, request for an ED bed, comorbidities, pregnancy status, frequency of ED visits ($>$ 2 times a week or $>$ 3 times a month), 72-hour unscheduled returns.
Unstructured data included clinical notes of chief complaint, and the triage dependence. Clinical notes for chief complaints were written in short sentences or words in both Chinese and English. While measuring vital signs during triage, nurse gathers information from the patient or, if needed, from accompanying family members or friends. Examples of clinical notes include statements like “abdominal pain and diarrhea started a few days ago,” “redness and pus in both hips,” and “generalized body pain, facial droop since a few days ago, and lower limb weakness after getting up at around 6 AM”. Based on the gathered information, the nurse selects the appropriate category from a computerized triage classification system to determine the patient’s final triage level. Triage dependence involves triage nurses quickly generating specific descriptive phrases by selecting options from a computerized list, covering the patient’s system classification, main symptoms, and key findings, including specific vital signs or pain scores. For example, “patient belongs to the nervous system category, presenting with dizziness/vertigo, positional, without other neurological symptoms,” or “patient belongs to the respiratory system category, presenting with shortness of breath, mild respiratory distress (SpO2: 92–94\%).” This process directly correlates with the determination of the triage level.
Final dispositions (e.g., admitted to intensive care unit (ICU) or ward, discharged, discharged against medical advice, expired, or escaped) were also recorded in EMRs. We handled categorical variables by converting them using one-hot encoding. This approach ensures that the categorical data are represented in a binary format, suitable for input into the machine learning models.\\

\textbf{Methods:} NLP is increasingly being used in the health care sector. In NLP, sophisticated algorithms and machine learning techniques are used to search, analyze, and interpret massive volumes of patient data and to extract valuable insights and meaningful concepts from clinical notes that were previously considered lost due to the textual nature of the data [27].
We processed the unstructured data using a series of NLP techniques. First, we performed data cleansing, which involved removing irrelevant information, standardizing formats, and handling missing or noisy data. This step included removing stopwords, punctuation, and irrelevant characters, as well as performing tokenization and lemmatization to standardize the text. Chinese word segmentation was performed in Jieba in accurate mode [28]. To improve the accuracy of the segmentation, we incorporated a specialized medical dictionary containing relevant medical terms (e.g., disease names, symptoms, and treatments) into Jieba, ensuring the accurate segmentation of medical terminology. The key variables in this stage were the words, terms, and segmented phrases, which represented important clinical terminology. which represented important clinical terms. The detailed list of each Chinese term and phrase extracted from unstructured data, along with their English translations, is provided in Supplementary Table 1. These variables were then encoded using one-hot encoding, where each word in the vocabulary was represented by a binary vector with a ‘1’ indicating the presence of that word in the text and a ‘0’ indicating its absence. This transformed the textual data into numerical vectors suitable for model input.
Additionally, we integrated structured data (e.g., patient demographics, lab results) with the encoded text features. These structured variables were preprocessed as follows: numerical variables (e.g., age, lab test results) were normalized, while categorical variables (e.g., gender, diagnosis category) were encoded using one-hot encoding. We concatenated the encoded text features and structured data into a single feature vector, which was then used as input to our machine learning model.\\

\textbf{Results:} For the primary outcome, although the boosting models demonstrated significantly better AUROC and F1 scores for predicting the primary outcome compared to other models, these three models (LGBM, CatBoost and GB) exhibit a notable overestimation of risk in their calibration. In contrast, while Random Forest did not achieve the highest AUROC, it yielded the highest F1 score of 0.500 and the lowest Brier score of 0.072. Besides, most models achieved higher F1 scores compared to the F1 score of 0.361 for EPs. Given the lower prevalence of the primary outcome, most models demonstrated higher specificity and negative predictive value. For the secondary outcome, compared to EPs and LR-TTAS, all models improved performance with Random Forest standing out with the highest AUC if 0.847 and the lowest Brier score of 0.089. LR-TTAS showed the poorest performance with a F1 score of 0 in the primary outcome and 0.171 in the secondary outcome. The DeLong test indicated that the AUROC of Random Forest was significantly higher than that of the other models. LR exhibits the largest deviation from perfect calibration, consistently overestimating risk across the probability range.
\newpage
\section{Experimental Methodology, Results, and SOTA Comparison}

\subsection{Data Description and Exploratory Data Analysis (EDA)}

\paragraph{Dataset Source and Access}
The models presented in this study were trained and evaluated using the \textbf{MIMIC-III (Medical Information Mart for Intensive Care III)} database, version 1.4. MIMIC-III is a large, freely-available database comprising de-identified health-related data associated with over 40,000 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.

Due to the sensitive nature of the data (PHI - Protected Health Information), access is restricted. Our team completed the required CITI Program "Data or Specimens Only Research" course and obtained authorized credentials to access the dataset, ensuring compliance with HIPAA regulations.

\paragraph{Data Construction and Preprocessing}
To simulate a realistic digital triage scenario, we constructed our dataset by extracting and concatenating unstructured text fields that typically represent a patient's initial presentation:
\begin{itemize}
    \item \textbf{Input ($X$):} A concatenation of the \texttt{Chief Complaint} and \texttt{History of Present Illness} columns.
    \item \textbf{Target ($y$):}
    \begin{itemize}
        \item For the \textit{Routing Module}: We utilized Natural Language Processing (NER) to extract medical entities and map them to 9 distinct medical specialties (e.g., Cardiology, Neurology).
        \item For the \textit{Risk Assessment Module}: We utilized the \texttt{Discharge Disposition} field, grouped into binary categories (Home vs. Admission/Critical) to serve as a proxy for severity.
    \end{itemize}
\end{itemize}

\paragraph{Exploratory Analysis and Challenges}
A preliminary Exploratory Data Analysis (EDA) revealed several critical characteristics that influenced our modeling strategy:

\begin{enumerate}
    \item \textbf{Significant Class Imbalance:} The distribution of medical specialties is highly skewed. Common categories such as \textit{General Medicine} and \textit{Cardiology} account for the majority of the samples, while specialties like \textit{Oncology} or \textit{Neurology} are underrepresented. This necessitated the implementation of Weighted Loss functions during training.
    \item \textbf{Text Sequence Length:} The analysis of token counts showed a high variance in note length. While some triage notes are concise ($<50$ tokens), others exceed the standard BERT limit of 512 tokens, requiring truncation strategies that preserve the initial ("Chief Complaint") segment.
    \item \textbf{Data "Noise" and Artifacts:} Initial inspection revealed parsing anomalies in the raw CSV files (e.g., column shifts due to unescaped commas). Furthermore, the text is rich in non-standard medical abbreviations (e.g., "pt", "hx", "c/o") and negations, which required a specialized tokenizer rather than standard text cleaning methods.
\end{enumerate}

\begin{table}[htbp]
    \centering
    \caption{Data Partitioning for Model Training}
    \label{tab:data_split}
    \begin{tabular}{l|c|c}
        \hline
        \textbf{Split} & \textbf{Percentage} & \textbf{Purpose} \\ \hline
        Training Set & 80\% & Model weights optimization \\
        Validation Set & 10\% & Hyperparameter tuning and early stopping \\
        Test Set & 10\% & Final performance evaluation on unseen data \\ \hline
    \end{tabular}
\end{table}

\paragraph{Overview of Model Architectures and Configurations}\mbox{}\\[0.5em]

To address the diverse challenges of clinical text classification—ranging from triage prediction to procedure categorization—we employed four distinct Transformer-based architectures. Each model was selected based on its specific pre-training strengths and adapted to a specific downstream task through fine-tuning.

Table \ref{tab:model_specs} summarizes the technical specifications, specialization domains, and target configurations for each model used in our experiments.

\begin{table}[htbp]
    \centering
    \caption{Summary of Model Architectures and Experimental Configurations}
    \label{tab:model_specs}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|l|l|l|c}
        \hline
        \textbf{Model Name} & \textbf{Base Architecture} & \textbf{Pre-training Specialization} & \textbf{Downstream Task} & \textbf{Output Classes} \\ \hline
        \textbf{ClinicalBERT} & BERT-Base & Clinical Text (MIMIC-III) & Triage (Discharge) & 20 \\
        \textbf{PubMedBERT} & BERT-Base & Biomedical (PubMed Abstracts) & Clinical Note Classification & 4 (Grouped) \\
        \textbf{RoBERTa} & RoBERTa-Base & General Domain (Robust) & Procedure Prediction & 101 \\
        \textbf{BioBERT} & BERT-Base & Biomedical (PubMed + PMC) & Medical Specialty Triage & 9 \\ \hline
    \end{tabular}%
    }
\end{table}

\subsection*{Model Specifications}

\paragraph{1. ClinicalBERT (emilyalsentzer/Bio\_ClinicalBERT)}
\begin{itemize}
    \item \textbf{Reference:} \href{https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT}{Hugging Face Repository} | \href{https://arxiv.org/abs/1904.03323}{Alsentzer et al., Publicly Available Clinical BERT Embeddings (2019)}
    \item \textbf{Specialization:} This model is initialized from BioBERT and further pre-trained on the MIMIC-III database containing intensive care unit notes. It is highly specialized in understanding raw clinical jargon, abbreviations, and hospital-specific context.
    \item \textbf{Model Size:} $\sim$110 Million parameters (12 layers, 768 hidden units).
    \item \textbf{Training Methodology:} We employed \textbf{Transfer Learning} by adding a fully connected classification head. The model was fine-tuned for 3 epochs on a subset of admission notes to predict patient discharge disposition.
    \item \textbf{Class Configuration:} The classification head is configured for \textbf{20 distinct classes} representing specific discharge locations (e.g., Home, SNF, Expired).
\end{itemize}

\paragraph{2. PubMedBERT (microsoft/BiomedNLP-PubMedBERT)}
\begin{itemize}
    \item \textbf{Reference:} \href{https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext}{Hugging Face Repository} | \href{https://arxiv.org/abs/2007.15779}{Gu et al., Domain-Specific Language Model Pretraining for Biomedical NLP (2020)}
    \item \textbf{Specialization:} Unlike models initialized from general BERT, PubMedBERT was pre-trained from scratch purely on biomedical text (PubMed abstracts and full-text articles). This vocabulary is optimized strictly for scientific and medical terminology.
    \item \textbf{Model Size:} $\sim$110 Million parameters (Standard BERT-Base architecture).
    \item \textbf{Training Methodology:} The model was fine-tuned using a standard Cross-Entropy Loss approach for 3 epochs, specifically focusing on extracting semantic meaning from concatenated clinical notes.
    \item \textbf{Class Configuration:} Used for multi-class classification of discharge dispositions, evaluated primarily on \textbf{4 aggregate categories} of severity.
\end{itemize}

\paragraph{3. RoBERTa (RoBERTa-Base)}
\begin{itemize}
    \item \textbf{Reference:} \href{https://huggingface.co/roberta-base}{Hugging Face Repository} | \href{https://arxiv.org/abs/1907.11692}{Liu et al., RoBERTa: A Robustly Optimized BERT Pretraining Approach (2019)}
    \item \textbf{Specialization:} A robustly optimized version of BERT trained on a massive corpus of general English text (160GB). While not domain-specific, its superior training methodology makes it highly effective at learning complex syntactic structures in high-dimensional tasks.
    \item \textbf{Model Size:} $\sim$125 Million parameters (slightly larger than BERT due to vocabulary size).
    \item \textbf{Training Methodology:} Fine-tuned for \textbf{Large-Scale Classification} using a weighted loss function to handle severe class imbalance. Training was conducted for 1 epoch as a feasibility study on a larger dataset (50k samples).
    \item \textbf{Class Configuration:} High-cardinality classification involving \textbf{101 classes} (Top 100 surgical procedures + 1 'Other' category).
\end{itemize}

\paragraph{4. BioBERT (dmis-lab/biobert-base-cased)}
\begin{itemize}
    \item \textbf{Reference:} \href{https://huggingface.co/dmis-lab/biobert-base-cased-v1.2}{Hugging Face Repository} | \href{https://arxiv.org/abs/1901.08746}{Lee et al., BioBERT: a pre-trained biomedical language representation model (2019)}
    \item \textbf{Specialization:} The standard for biomedical NLP, pre-trained on PubMed abstracts and PubMed Central full-text articles. It serves as a bridge between general language understanding and biomedical entities.
    \item \textbf{Model Size:} $\sim$110 Million parameters.
    \item \textbf{Training Methodology:} The pipeline involved a two-stage process: first generating labels via NER, then fine-tuning BioBERT to route patients to specialties.
    \item \textbf{Class Configuration:} Configured for \textbf{9 medical specialties} (e.g., Cardiology, Neurology, General Medicine) to simulate a digital triage routing system.
\end{itemize}

\subsection{ClinicalBERT: Experimental Methodology and Results}

The initial experimental goal was to validate the feasibility of using a Transformer model for clinical text-based triage prediction. We employed \textbf{Transfer Learning} using \texttt{emilyalsentzer/Bio\_ClinicalBERT}, a model pre-trained on vast amounts of medical and clinical text (e.g., PubMed and MIMIC-III notes), which already possesses a strong understanding of medical terminology and context.

\subsubsection*{Data Used}
\begin{itemize}
    \item \textbf{Input Data ($X$):} Textual data from the \texttt{chief\_complaint} and \texttt{history\_of\_present\_illness} columns, concatenated into a single input string.
    \item \textbf{Target Data ($y$):} The \texttt{discharge\_disposition} column, used as a \textbf{proxy for patient severity/triage level}:
    \begin{itemize}
        \item \texttt{Home} $\implies$ Low Severity
        \item \texttt{Home With Service Facility} $\implies$ Medium Severity
        \item \texttt{Extended Care Facility} $\implies$ High Severity
        \item \texttt{Expired} $\implies$ Critical Severity
    \end{itemize}
    \item \textbf{Dataset Sample:} A small, imbalanced sample was used for the proof-of-concept: 5,000 training rows, 500 validation rows, and 500 test rows.
\end{itemize}

\subsubsection*{Model Training}
A simple classification head (fully connected layer) was added on top of the pre-trained ClinicalBERT base. The model was \textbf{fine-tuned} for 3 epochs to specialize in mapping the admission text to one of the 20 distinct \texttt{discharge\_disposition} classes. The validation set was used for early stopping, with the \texttt{load\_best\_model\_at\_end} parameter ensuring that the best-performing model (based on validation loss) was saved.

\subsubsection*{Results}
The final evaluation was conducted on the 500-row unseen test set.

\paragraph{Training and Validation Summary}
The model performance peaked at \textbf{Epoch 2}, indicating the onset of overfitting at Epoch 3, which was mitigated by loading the best model.

\begin{itemize}
    \item \textbf{Epoch 1:} Training Loss 1.1150, Validation Loss 1.1906, Accuracy 53.6\%, F1 = 0.523
    \item \textbf{Epoch 2 (Best):} Training Loss 1.0754, Validation Loss 1.1048, Accuracy 57.8\%, F1 = 0.548
    \item \textbf{Epoch 3:} Training Loss 1.0981, Validation Loss 1.1496, Accuracy 57.6\%, F1 = 0.554
\end{itemize}

\paragraph{Final Test Performance}
\textbf{Overall Test Accuracy: 58\%}

\begin{itemize}
    \item \textbf{Strengths:} High accuracy for \texttt{Home} (F1 = 0.75), and reasonable performance for \texttt{Extended Care Facility} (F1 = 0.54).
    \item \textbf{Weaknesses:} Poor recall for rare but critical classes such as \texttt{Expired} (F1 = 0.00) and \texttt{Home With Service Facility} (F1 = 0.30).
    \item \textbf{Critical Failure Example:} A severe case (e.g., ``chest pain and difficulty breathing'') misclassified as \texttt{Home} (45.5\% confidence).
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{confusion_matrix_paul_it3.png.jpg}
    \caption{Confusion Matrix for ClinicalBERT Triage Prediction (Test Set, 500 samples)}
    \label{fig:conf_matrix_ext}
\end{figure}

\subsubsection*{Comparison with State-of-the-Art (SOTA)}
The obtained results (\textbf{58\% Accuracy}) are currently below the State-of-the-Art benchmarks. However, a granular comparison reveals that this gap is largely driven by data scarcity and differences in input data structure (structured questionnaires vs. unstructured text).

\paragraph{Detailed SOTA Benchmarks}
We compare our ClinicalBERT proof-of-concept against two leading systems described in the Related Work section:

\begin{itemize}
    \item \textbf{Triage24 (Platform24) - Tree-Based Models}
    \begin{itemize}
        \item \textbf{Reference:} \href{https://arxiv.org/pdf/2504.11977}{Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews}
        \item \textbf{Performance:} \textbf{88.2\% Accuracy} (LGBMClassifier).
        \item \textbf{Data Distribution:} Trained on \textbf{330,000+ triage interviews}.
        \item \textbf{Comparison Context:} While their accuracy is higher, their data consists of \textit{structured} questionnaire responses. Our model tackles a harder problem: extracting signals from \textit{unstructured, noisy} clinical notes with only 6,000 samples.
    \end{itemize}

    \item \textbf{URGENTIAPARSE - LLM for Emergency Triage}
    \begin{itemize}
        \item \textbf{Reference:} \href{https://bmcemergmed.biomedcentral.com/articles/10.1186/s12873-024-01152-1/}{Using machine learning... in triage for prediction of clinical disposition}
        \item \textbf{Performance:} \textbf{F1-Score = 0.900}.
        \item \textbf{Data Distribution:} Retrospective data from a French Emergency Department (Adults). The distribution is highly similar to our target (ED Triage), but the study utilizes Large Language Models (LLMs) which are significantly more resource-intensive than our lightweight ClinicalBERT implementation.
    \end{itemize}
\end{itemize}

\paragraph{Analysis of the Performance Gap}
The performance difference (58\% vs. $\approx$89\%) is attributed to three specific factors:
\begin{enumerate}
    \item \textbf{Data Volume:} SOTA models are trained on datasets ranging from \textbf{100k to 7 million} samples. Our proof-of-concept utilized only \textbf{6,000 samples}, limiting the model's ability to generalize on rare classes.
    \item \textbf{Input Complexity:} Triage24 benefits from clean, structured inputs. Our model must first parse noisy text (abbreviations, typos) before classification.
    \item \textbf{Class Balance:} The SOTA datasets often undergo rigorous balancing or have naturally larger representations of minority classes, whereas our subset remained highly imbalanced.
\end{enumerate}


\subsubsection*{Conclusion and Next Steps}
This experiment is a successful \textbf{proof-of-concept}. The low recall on rare classes is due to limited data, not model flaws.  
\textbf{Next step:} Retrain on the complete dataset to reduce bias and approach SOTA performance (85\%+).

\subsection{PubMedBERT: Clinical Note Classification}

\subsubsection*{Summary and Objective}
In this module, we developed a machine learning-enhanced system to process clinical discharge notes and predict patient discharge disposition. The pipeline consists of preprocessing textual data from clinical notes into a single cleaned string, tokenizing using the specialized PubMedBERT tokenizer, and fine-tuning the model for multi-class classification.

\subsubsection*{Dataset and Input Configuration}
For this experiment, we utilized a subset of \textbf{6,000 records} from the MIMIC-III dataset, partitioned as follows:
\begin{itemize}
    \item \textbf{Training Set:} 5,000 rows
    \item \textbf{Validation Set:} 500 rows
    \item \textbf{Test Set:} 500 rows
\end{itemize}

\paragraph{Feature Selection}
Unlike previous iterations that used all available columns, this optimized experiment focused on the two most information-dense fields:
\begin{itemize}
    \item \textbf{Input ($X$):} Concatenated text from \texttt{chief\_complaint} and \texttt{history\_of\_present\_illness}.
    \item \textbf{Target ($y$):} \texttt{discharge\_disposition} (e.g., Home, Home with Service Facility, Expired, Extended Care Facility).
\end{itemize}

\subsubsection*{Model Architecture: PubMedBERT}
We employed \textbf{PubMedBERT} (\texttt{microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext}).

\paragraph{Why PubMedBERT?}
Unlike general-purpose models (BERT) or domain-adapted models (BioBERT, ClinicalBERT) which are initialized from general text, PubMedBERT was **pre-trained from scratch** solely on biomedical text corpora (PubMed abstracts and full-text articles).
\begin{itemize}
    \item \textbf{Architecture:} 12 transformer layers, 768 hidden dimensions, $\sim$110 million parameters.
    \item \textbf{Vocabulary:} Optimized strictly for specialized medical and scientific terminology.
    \item \textbf{Task:} Sequence classification (Discharge Disposition Prediction).
\end{itemize}

\subsubsection*{Training Configuration}
The model was trained using the \texttt{HuggingFace Trainer} with the following hyperparameters:
\begin{itemize}
    \item \textbf{Batch Size:} 8
    \item \textbf{Max Sequence Length:} 512 tokens
    \item \textbf{Learning Rate:} $5e^{-5}$
    \item \textbf{Epochs:} 3
    \item \textbf{Loss Function:} Cross-Entropy Loss
\end{itemize}

\paragraph{Training Dynamics}
The training completed for 3 epochs with the following final metrics:
\begin{itemize}
    \item \textbf{Final Training Loss:} 0.5566
    \item \textbf{Gradient Norm:} 14.651
\end{itemize}

\subsubsection*{Final Performance Evaluation}
The model was evaluated on the unseen test set (500 samples).

\paragraph{Quantitative Metrics}
The PubMedBERT model achieved reasonable performance given the small dataset size:
\begin{itemize}
    \item \textbf{Accuracy:} \textbf{0.63}
    \item \textbf{F1-score:} \textbf{0.59}
    \item \textbf{Precision:} 0.58
    \item \textbf{Recall:} 0.63
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{classification_report_Alex_it3.png}
    \caption{Classification Report for PubMedBERT (Test Set)}
    \label{fig:pubmed_report}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{confusion_matrix_Alex_it3.png}
    \caption{Confusion Matrix of PubMedBERT predictions vs. true labels.}
    \label{fig:pubmed_conf}
\end{figure}

\subsubsection*{Conclusion}
The PubMedBERT model demonstrated the ability to classify discharge dispositions from unstructured clinical notes with an \textbf{Accuracy of 63\%} and an \textbf{F1-score of 0.59}.

\paragraph{Future Work}
To bridge the gap between the current performance and SOTA benchmarks, the following steps are proposed:
\begin{itemize}
    \item \textbf{Full Dataset Training:} Scale up from 6,000 rows to the full dataset to improve generalization.
    \item \textbf{Hyperparameter Tuning:} Experiment with learning rate schedules and batch sizes.
    \item \textbf{Data Augmentation:} Apply advanced preprocessing or synthetic data generation for rare classes.
\end{itemize}

\subsection{RoBERTa: Discharge Disposition Prediction (Standardized)}

\subsubsection*{Objective and Methodology}
To ensure a robust "apples-to-apples" comparison with the ClinicalBERT and PubMedBERT experiments, we aligned the experimental setup for the RoBERTa model. Instead of predicting surgical procedures, we configured RoBERTa to predict the **Discharge Disposition** on the exact same dataset sample size (5,000 training rows).

\paragraph{Dataset and Target Variable}
\begin{itemize}
    \item \textbf{Input Data ($X$):} Concatenated text from \texttt{chief\_complaint} and \texttt{history\_of\_present\_illness}.
    \item \textbf{Target Data ($y$):} The \texttt{discharge\_disposition} column.
    \item \textbf{Target Classes:} Following a strict data cleaning and mapping process to remove parsing artifacts, we consolidated the target variable into \textbf{5 distinct classes}:
    \begin{enumerate}
        \item \texttt{Home}: Routine discharge.
        \item \texttt{Extended Care}: Includes Rehab and Skilled Nursing Facilities.
        \item \texttt{Long Term Care}: Includes Nursing Facilities and Home Health Care.
        \item \texttt{Expired}: Critical outcome / Patient Deceased.
        \item \texttt{nan}: Missing or Unknown disposition (treated as a distinct class).
    \end{enumerate}
    \item \textbf{Dataset Sample:} We strictly adhered to the standardized split used in previous experiments:
    \begin{itemize}
        \item \textbf{Training Set:} 5,000 samples.
        \item \textbf{Test Set:} 500 samples (unseen data).
    \end{itemize}
\end{itemize}

\paragraph{Model Training and Configuration}
\begin{itemize}
    \item \textbf{Model:} \texttt{roberta-base} (125M parameters).
    \item \textbf{Loss Function:} Weighted Cross-Entropy Loss was applied to address the class imbalance.
    \item \textbf{Hyperparameters:} Trained for \textbf{3 Epochs} with a batch size of 8 and gradient accumulation.
\end{itemize}

\subsubsection*{Results and Discussion}
The model was evaluated on the fixed 500-sample test set.

\paragraph{Quantitative Metrics}
\begin{itemize}
    \item \textbf{Accuracy:} \textbf{51.60\%}
    \item \textbf{Weighted F1-score:} \textbf{0.50}
\end{itemize}

\paragraph{Comparative Analysis}
These results provide a critical validation of the importance of domain-specific pre-training. RoBERTa, trained on general English text (Web, News), achieved the lowest performance ($\mathbf{51.6\%}$), lagging behind **ClinicalBERT ($\mathbf{58\%}$)** and significantly behind **PubMedBERT ($\mathbf{68.4\%}$)**. This confirms that for interpreting triage notes, general language models struggle with clinical semantics compared to biologically pre-trained models.

\paragraph{Confusion Matrix Analysis}
Figure \ref{fig:roberta_conf} illustrates the model's performance. The model shows moderate capability in identifying the majority class (\texttt{Home}) but struggles significantly to distinguish between semantically similar categories like \texttt{Extended Care} and \texttt{Long Term Care}, often confusing them. This confusion is expected given the lack of specialized medical vocabulary in the base RoBERTa model.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{confusion_matrix_delia_it3.png}
    \caption{Confusion Matrix for RoBERTa (Discharge Disposition). The accuracy of 51.6\% highlights the limitations of general-domain models in specialized clinical classification compared to PubMedBERT.}
    \label{fig:roberta_conf}
\end{figure}

\subsubsection*{Conclusion}
This experiment serves as a negative control, validating our hypothesis: **General-purpose Transformers (RoBERTa) are suboptimal for clinical triage**. While capable of learning basic patterns, they are outperformed by domain-specific models (PubMedBERT) which should be the preferred architecture for the final system.

\subsection{BioBERT: Specialized Classification and Triage}

\subsubsection*{Objective and Methodology}
The primary objective was to validate a \textbf{digital medical triage pipeline} by classifying admission notes to the most appropriate of \textbf{9 specialty domains}. We utilized a specialized model, \textbf{BioBERT-Base-Cased}, to extract semantic information from the patient's clinical text.

The task involved a two-stage approach: generating the labels using NER, followed by fine-tuning the model to predict these labels, establishing the foundation for a patient routing system.

\paragraph{Dataset and Target Variable}
\begin{itemize}
    \item \textbf{Input Data ($X$):} Concatenation and preprocessing of the \texttt{chief\_complaint} and \texttt{history\_of\_present\_illness} columns.
    \item \textbf{Label Generation:} The specialist label (e.g., "Cardiology", "Pulmonology") was generated heuristically by extracting medical entities (diseases, symptoms) using an \textbf{NER pipeline} (based on \texttt{d4data/biomedical-ner-all}) from the \texttt{brief\_hospital\_course} and \texttt{discharge\_instructions} fields.
    \item \textbf{Target Classes:} This resulted in \textbf{9 specialty classes} (Infectious-Disease, Cardiology, Orthopedics-Surgery, Oncology, Pulmonology, Endocrinology, Gastroenterology, Neurology), with \texttt{General\_Medicine} serving as the \textit{fallback} class.
    \item \textbf{Sample Dataset:} A sample of $\mathbf{5,000}$ training rows, $\mathbf{500}$ validation rows, and $\mathbf{500}$ test rows was used for fine-tuning to validate the pipeline.
\end{itemize}

\paragraph{Model Training and Configuration}
The \textbf{BioBERT-Base-Cased (dmis-lab/biobert-base-cased-v1.2)} model was fine-tuned with a standard classification head.

\begin{itemize}
    \item \textbf{Optimizer:} \textbf{AdamW} with a learning rate of $\mathbf{2e-5}$, optimal for Transformer model fine-tuning.
    \item \textbf{Effective Batch Size:} $\mathbf{BATCH\_SIZE=8}$ was used on a CPU (or MPS) device, maintaining stable training.
    \item \textbf{Epochs:} The model was trained for \textbf{2 Epochs}.
    \item \textbf{Evaluation Metric:} Given the inherent data imbalance (some specialties are more frequent), the primary metric was the \textbf{Macro F1-score}, providing a more equitable measure of performance across all 9 classes.
\end{itemize}

\subsubsection*{Results and Discussion}

The model was evaluated after 2 training epochs on the 5,000-sample set and tested on the unseen 500-sample test set.

\paragraph{Training and Validation Summary (2 Epochs)}

\paragraph{Final Test Performance (9 Classes)}
Evaluation on the unseen test set provided the following final metrics (data to be completed):

\begin{verbatim}
{'eval_accuracy': 0.55, 'eval_f1_macro': 0.17, ...}
\end{verbatim}

\begin{itemize}
    \item \textbf{Accuracy:} $\mathbf{55\%}$
    \item \textbf{Macro F1-score:} $\mathbf{0.17}$
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{confusion_matrix_denis_iter2.png}
\end{figure}

\paragraph{Analysis}
\begin{itemize}
    \item \textbf{Learning Capability:} An accuracy of $\mathbf{[Acc\_test]\%}$ (compared to $\approx 11\%$ for a random guess) confirms that BioBERT successfully extracted and associated medical semantic patterns (symptoms, conditions) with the corresponding specialty.
    \item \textbf{Impact of Imbalance:} The Macro F1-score indicates that while the model performs well on frequent specialties (e.g., General Medicine, Cardiology), \textbf{rare classes (e.g., Oncology, Neurology)} are insufficiently represented.
    \item \textbf{Pipeline Validation:} The NER-based label generation was validated, and the BioBERT model proved to be the correct architecture for understanding the medical context.
\end{itemize}

\subsubsection*{Conclusion and Next Steps}
The experiment successfully validated the entire pipeline, from label generation to fine-tuning a BioBERT model for \textbf{specialty classification}. The current limitation is the small and imbalanced size of the training dataset.

\paragraph{Future Work}
\begin{itemize}
    \item \textbf{Full Training:} Extend training to $\mathbf{3-5}$ epochs on the full dataset (without subsampling) to allow the model to learn the subtle patterns of rare classes.
    \item \textbf{Weighted Classes (Recommended):} Although not strictly necessary with the small sample, implementing a \textbf{weighted loss function} (similar to the RoBERTa experiment) is crucial at scale to mitigate the impact of rare classes.
    \item \textbf{Human Label Evaluation:} Manually verify a sample of the NER-generated labels to quantify the error in the target variable ($y$) generation.
\end{itemize}

\newpage

\section{Enhancement of the Intelligent Algorithm}

Building upon the proof-of-concept experiments, this chapter focuses on optimizing the architecture and, crucially, ensuring data integrity. The primary goal was to transition to full-scale training while correcting data ingestion artifacts identified during the preliminary analysis.

\subsection{Optimizing RoBERTa: Data Correction and Full-Scale Training}

\subsubsection*{Methodological Corrections and Improvements}
During the expansion to the full dataset (approx. 250,000 records), a critical audit of the training data revealed a parsing anomaly in the initial experiments. The previous CSV ingestion method incorrectly interpreted commas within clinical text fields, resulting in a column shift where pharmaceutical instructions (e.g., "take 1 tablet") were treated as target labels.

To achieve valid, production-level performance, the following rigorous optimizations were implemented:

\begin{itemize}
    \item \textbf{Data Ingestion Repair:} We implemented a robust parsing engine capable of handling quoted strings within the CSV files. This corrected the column alignment, ensuring that the \texttt{major\_surgical\_procedure} column contained actual surgical interventions (e.g., \textit{Laparoscopic Appendectomy}, \textit{Colonoscopy}) rather than unrelated text.
    \item \textbf{Label Cleaning and Filtering:} The target labels were sanitized to remove non-procedural noise. We focused on the \textbf{Top 100 most frequent surgical procedures}, grouping all other rare interventions into an \texttt{other\_procedure} class.
    \item \textbf{Full Dataset Utilization:} We utilized the complete cleaned dataset for training (3 Epochs), allowing the model to learn from the full variance of clinical presentations.
\end{itemize}

\subsubsection*{Experimental Results}
The optimized RoBERTa model was evaluated on the unseen test set using the corrected labels. Unlike the initial artifactual results, the current metrics reflect the model's genuine ability to interpret medical semantics.

\paragraph{Quantitative Metrics}
\begin{itemize}
    \item \textbf{Test Accuracy:} \textbf{55.73\%} (Reflecting real-world performance on 101 distinct classes).
    \item \textbf{Weighted F1-Score:} \textbf{59.83\%}
\end{itemize}

\paragraph{Confusion Matrix Analysis}
The confusion matrix (Figure \ref{fig:roberta_real}) demonstrates strong predictive capabilities along the diagonal, confirming that the model effectively links patient complaints to the correct surgical intervention.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{cm2_delia.png}
    \caption{Confusion Matrix for RoBERTa on Corrected Data. The diagonal indicates correct classification of distinct procedures. A cluster of confusion is visible among variations of 'None', indicating an opportunity for label consolidation.}
    \label{fig:roberta_real}
\end{figure}

\subsubsection*{Discussion and Analysis}
\begin{itemize}
    \item \textbf{True Semantic Learning:} The model successfully distinguishes between distinct physiological systems. For instance, inputs describing "RLQ pain" are correctly mapped to \texttt{laparoscopic appendectomy}, while "chest pain" inputs map to \texttt{cardiac catheterization}. This validates the system's utility as a triage engine.
    \item \textbf{The "None" Ambiguity:} As observed in the bottom-right quadrant of the confusion matrix, the model struggles to differentiate between synonymous negative labels such as \texttt{none}, \texttt{none.}, \texttt{none during this admission}, and \texttt{none this hospitalization}. These labels represent the same semantic concept (no procedure performed) but are treated as separate classes.
\end{itemize}

\subsubsection*{Conclusion}
The correction of the data pipeline has resulted in a trustworthy and validated model. While the numerical accuracy is lower than the artifactual baseline, it represents \textbf{real clinical value}. The identification of the "None" class ambiguity provides a clear path for the final optimization: consolidating all negative variations into a single \texttt{NO\_PROCEDURE} class will significantly boost the final system performance.

\subsection{Optimizing BioBERT: Advanced Preprocessing and Semantic Labeling}

\subsubsection*{Methodological Corrections and Improvements}
Following the initial baseline experiments, the BioBERT fine-tuning pipeline underwent a significant architectural overhaul. The dataset was scaled up by a factor of approximately 20, providing a substantial corpus for training over 5 epochs. To handle this complexity and address critical flaws in the preliminary approach, the following optimizations were implemented:

\begin{itemize}
    \item \textbf{Refined Text Preprocessing:} The initial preprocessing strategy, which filtered out tokens based on length and strictly alphanumeric criteria, was found to be detrimental to medical semantics.
    \begin{itemize}
        \item \textit{Negation Handling:} We ceased the removal of "irrelevant" words that included negative particles. Preserving terms like "no", "not", or "denies" is crucial to prevent semantic distortion (e.g., distinguishing "chest pain" from "no chest pain").
        \item \textit{Abbreviation Preservation:} The filter eliminating tokens with a length $\le 2$ was removed. In the clinical domain, short abbreviations such as \textit{MI} (Myocardial Infarction), \textit{PE} (Pulmonary Embolism), or \textit{ER} (Emergency Room) carry high-density information essential for correct classification.
    \end{itemize}

    \item \textbf{Semantic Labeling via BioBERT Similarity:} We replaced the rigid dictionary-based weak supervision method, which relied on the first matching term and lacked scalability. Instead, we implemented a \textbf{BioBERT Similarity} mechanism. We generated embeddings for both the patient input and concise descriptions of each medical specialization. The target label is now assigned based on the highest \textbf{Cosine Similarity} score between these vector representations, ensuring a context-aware rather than keyword-based classification.

    \item \textbf{Training Stability and Class Imbalance:}
    \begin{itemize}
        \item \textit{Weighted Cross-Entropy Loss:} To address the inherent class imbalance typical of medical datasets, we transitioned from standard Cross-Entropy to a Weighted Cross-Entropy Loss function. This penalizes misclassifications of rare classes more heavily, preventing the model from biasing towards the majority class.
        \item \textit{Variable Learning Rate:} We introduced a learning rate scheduler with a warmup period followed by linear decay. This prevents "catastrophic forgetting" of pre-trained knowledge early in training and ensures stable convergence in later epochs.
    \end{itemize}
\end{itemize}

\subsubsection*{Experimental Results}
The optimized BioBERT model was evaluated on the significantly expanded test set. The incorporation of semantic labeling and improved preprocessing has yielded metrics that better reflect the model's ability to handle complex clinical narratives.

\paragraph{Quantitative Metrics}
\begin{itemize}
    \item \textbf{Test Accuracy:} \textbf{77.37\%}
    \item \textbf{Weighted F1-Score:} \textbf{77.52\%}
\end{itemize}

\paragraph{Confusion Matrix Analysis}
As illustrated in Figure \ref{fig:biobert_cm}, the confusion matrix highlights the impact of the weighted loss function on minority classes.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{cm4_denis.png}
    \caption{Confusion Matrix for the optimized BioBERT model. The results demonstrate the effectiveness of semantic similarity labeling in reducing noise compared to the dictionary-based baseline.}
    \label{fig:biobert_cm}
\end{figure}

\subsection*{Operational Efficiency and Model Explainability}

Beyond predictive accuracy, the deployment viability of the BioBERT triage system was evaluated against critical operational metrics: inference latency, computational footprint, and decision transparency.

\subsubsection*{Computational Efficiency Profile}
To assess the system's suitability for real-time clinical environments, we benchmarked the model on a \textbf{supercomputer}.
\begin{itemize}
    \item \textbf{Inference Latency:} The average processing time per patient record is \textbf{4.78 ms}. This near-instantaneous response time ensures that the triage engine does not introduce bottlenecks in the hospital admission workflow.
    \begin{itemize}
        \item \textbf{Model Footprint:} The fine-tuned model occupies approximately \textbf{413.27 MB} of disk space with \textbf{108.32 million parameters}. This creates a balanced trade-off between the semantic depth of a large language model and the resource constraints of hospital IT infrastructure.
    \end{itemize}
\end{itemize}

\subsubsection*{Explainability and Trust (XAI)}
In medical AI, a "black-box" approach is unacceptable due to liability and trust concerns. To ensure transparency, we employed \textbf{SHAP (SHapley Additive exPlanations)} to generate local explanations for individual predictions.

As demonstrated in the explainability analysis, the model exhibits correct attention mechanisms. For example, in a case diagnosed as \texttt{Neurology}, the SHAP values assign high positive attribution to terms like \textbf{"headache"} and \textbf{"numb"} while correctly ignoring administrative stopwords. This confirms that the model's high F1-score is driven by genuine clinical feature detection rather than dataset artifacts.

\subsubsection*{Discussion and Analysis}
\begin{itemize}
    \item \textbf{Impact of Context Preservation:} By retaining short abbreviations and negation markers, the model shows improved performance on short, high-urgency inputs (e.g., "pt with hx of MI"). This confirms that standard NLP stop-word removal strategies are often ill-suited for clinical text.
    \item \textbf{Scalability of Semantic Labeling:} The shift to BioBERT Similarity has eliminated the need for manual dictionary maintenance. The model can now generalize to synonyms and related concepts that were not explicitly hardcoded, proving that embedding-based supervision is superior for scaling to larger medical datasets.
    \item \textbf{Handling Imbalance:} The use of Weighted Cross-Entropy has allowed the model to maintain sensitivity towards rarer specialties, which previously suffered from low recall in the unweighted training iterations.
\end{itemize}

\subsubsection*{Conclusion}
The transition to a semantic-based pipeline with domain-specific preprocessing has transformed the BioBERT implementation from a keyword-matching system into a robust semantic classifier. The use of variable learning rates and weighted loss functions has stabilized the training process on the large-scale dataset, providing a solid foundation for real-world triage application.

\subsection{Optimizing ClinicalBERT: Scalability and Resource Management}

\subsubsection*{Methodological Corrections and Improvements}
To address the limitations of the initial proof-of-concept, the ClinicalBERT pipeline was scaled to utilize the full training dataset ($\approx 234,000$ samples). This massive increase in data volume required significant architectural changes to ensure convergence stability and manage GPU resource constraints.

We implemented advanced training strategies within the \texttt{Hugging Face Trainer} to transition from a lightweight experiment to a production-grade training loop:

\begin{itemize}
    \item \textbf{Mixed Precision Training (FP16):} We transitioned from FP32 (Full Precision) to FP16. This reduced the memory footprint by approximately 50\%, allowing for larger batch sizes and faster computation on Tensor Core GPUs without degrading model accuracy.
    \item \textbf{Gradient Accumulation:} To stabilize the gradient updates, we implemented a virtual batching strategy. By accumulating gradients over 2 steps with a per-device batch size of 16, we achieved an \textbf{effective batch size of 32}. This mimics the stability of high-end server training on consumer-grade hardware.
    \item \textbf{Dynamic Stopping Strategy:} The rigid 3-epoch limit was replaced with a dynamic approach (up to 10 epochs) utilizing \textbf{Early Stopping} with a patience of 3. This allowed the model to train as long as it was learning, preventing both underfitting and overfitting.
\end{itemize}

\subsubsection*{Experimental Results}
The optimized model, trained on the full dataset, demonstrated clear improvements in generalization capabilities over the baseline.

\paragraph{Quantitative Metrics}
\begin{itemize}
    \item \textbf{Test Accuracy:} \textbf{63.3\%} (vs. 57.8\% in baseline)
    \item \textbf{Weighted F1-Score:} \textbf{0.59} (vs. 0.54 in baseline)
    \item \textbf{Validation Loss:} Decreased to \textbf{1.04}, indicating better fit.
\end{itemize}

\subsubsection*{Discussion}
While the performance improved, the gain was incremental rather than exponential. This suggests that while more data helps, ClinicalBERT (which is initialized from BioBERT but trained on MIMIC notes) might still struggle with the specific nuances of *triage* classification compared to pure procedure identification.
\subsection{PubMedBERT: Domain Specificity and Hyperparameter Optimization}

\subsubsection*{Objective and Methodology}
To rigorously test the hypothesis that **domain-specific vocabulary** is critical for medical text classification, we utilized \textbf{PubMedBERT} (\texttt{microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext}). Unlike ClinicalBERT (which initializes from general BERT), PubMedBERT is pre-trained \textit{from scratch} on biomedical abstracts, offering a vocabulary optimized strictly for scientific terminology.

\paragraph{Task Definition}
The model was fine-tuned to predict the patient's \texttt{discharge\_disposition} based on the concatenation of \texttt{chief\_complaint} and \texttt{history\_of\_present\_illness}. The target labels were normalized into \textbf{4 distinct classes} of severity:
\begin{enumerate}
    \item \textbf{Home:} Routine discharge without formal services.
    \item \textbf{Home With Service Facility:} Requires home health or formal support.
    \item \textbf{Extended Care Facility:} Transfer to skilled nursing/rehab.
    \item \textbf{Expired:} Patient deceased during hospital stay.
\end{enumerate}

\subsubsection*{Experimental Evolution}
We conducted two distinct training iterations to optimize performance and stability.

\paragraph{Experiment 1 (Baseline):} Trained on 100,000 samples for 2 epochs with a learning rate of $5e^{-5}$ and an effective batch size of 16.
\paragraph{Experiment 2 (Optimized):} To improve generalization, we scaled the training data to \textbf{200,000 samples} and increased the training duration to \textbf{3 epochs}. Crucially, we optimized the stability by:
\begin{itemize}
    \item \textbf{Lowering Learning Rate:} From $5e^{-5}$ to $\mathbf{2e^{-5}}$.
    \item \textbf{Increasing Effective Batch Size:} From 16 to \textbf{32} (using gradient accumulation).
\end{itemize}

\subsubsection*{Results and Discussion}
The optimization strategies in Experiment 2 yielded superior convergence. While Experiment 1 plateaued at an F1-score of $\approx 0.65$, the optimized model achieved a lower validation loss ($\mathbf{0.7522}$) and higher stability.

\paragraph{Final Quantitative Metrics (Optimized Model)}
\begin{itemize}
    \item \textbf{Accuracy:} \textbf{68.4\%}
    \item \textbf{Weighted F1-Score:} \textbf{0.676}
    \item \textbf{Precision:} 0.673
    \item \textbf{Recall:} 0.684
\end{itemize}

\paragraph{Confusion Matrix Analysis}
The classification report (Figure \ref{fig:pubmed_report}) and confusion matrix indicate that PubMedBERT significantly outperforms general-domain models. Notably, it shows improved capability in distinguishing nuanced categories (e.g., "Home" vs. "Home with Service"), a common failure point for standard BERT models.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{confusion_matrix_Alex_it4.png}
    \caption{Confusion Matrix for PubMedBERT (Experiment 2). The model demonstrates strong diagonal density, confirming high accuracy on the dominant classes.}
    \label{fig:pubmed_conf}
\end{figure}

\subsubsection*{Conclusion}
The PubMedBERT experiment validates that a model pre-trained on biomedical corpora, when fine-tuned with a stable hyperparameter configuration (lower learning rate, larger batch size), offers the best performance for clinical note classification. It achieved an accuracy of \textbf{68.4\%}, significantly outperforming the ClinicalBERT baseline ($\approx 58\%$).
\section{Conclusion and Architectural Integration}

The evaluation of PubMedBERT demonstrates its superior capability in handling the subtle nuances of biomedical outcomes compared to general clinical models. With an \textbf{Accuracy of 0.684} and an \textbf{F1-score of 0.671} on the discharge disposition task, PubMedBERT effectively functions as a proxy for patient severity (distinguishing between routine discharges and critical outcomes).

\subsection{Final Proposed System Architecture}
Based on the extensive experimental results across all tested models (RoBERTa, BioBERT, ClinicalBERT, and PubMedBERT), we propose a **Dual-Model Triage Architecture** to power the Digital Triage System. This approach leverages the specific strengths of the top-performing models:

\begin{itemize}
    \item \textbf{Module A: The "Router" (Action Engine) - BioBERT}
    \begin{itemize}
        \item \textbf{Function:} Determines \textit{"What to do / Where to go"}.
        \item \textbf{Justification:} BioBERT achieved the highest performance (\textbf{77.37\% Accuracy}) in the specialty classification task. It will be responsible for routing the patient to the correct medical department (e.g., Cardiology, Neurology, Gastroenterology).
    \end{itemize}

    \item \textbf{Module B: The "Assessor" (Severity Engine) - PubMedBERT}
    \begin{itemize}
        \item \textbf{Function:} Determines \textit{"How difficult/severe the case is"}.
        \item \textbf{Justification:} By predicting the \texttt{discharge\_disposition}, PubMedBERT effectively stratifies patients based on expected complexity. A prediction of "Home" implies low severity, whereas "Extended Care" or "Expired" implies high severity/criticality. This allows the system to prioritize cases within the correct department.
    \end{itemize}
\end{itemize}

\subsubsection*{Summary of Impact}
By decoupling the routing logic from the severity assessment, this hybrid approach minimizes the risk of bottlenecking critical patients in the wrong queue. The system not only directs the patient to the right specialist (via BioBERT) but also alerts the provider regarding the anticipated resource consumption and urgency (via PubMedBERT).

\end{document}